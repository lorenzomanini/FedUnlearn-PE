{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KXjLp3FDbodi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "#os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
        "import copy\n",
        "\n",
        "from utils.model import FLNet\n",
        "from utils.local_train import LocalTraining\n",
        "from utils.utils import Utils\n",
        "from utils.fusion import Fusion, FusionAvg, FusionRetrain\n",
        "\n",
        "from torch import (\n",
        "    allclose,\n",
        "    cat,\n",
        "    cuda,\n",
        "    device,\n",
        "    int32,\n",
        "    linspace,\n",
        "    manual_seed,\n",
        "    rand,\n",
        "    rand_like,\n",
        ")\n",
        "from torch.nn import (\n",
        "    Conv2d,\n",
        "    CrossEntropyLoss,\n",
        "    Flatten,\n",
        "    Identity,\n",
        "    Linear,\n",
        "    Module,\n",
        "    MSELoss,\n",
        "    ReLU,\n",
        "    Sequential,\n",
        ")\n",
        "\n",
        "from torch.nn.functional import cross_entropy, relu\n",
        "from torchvision.models import resnet18, resnet50\n",
        "\n",
        "from backpack import backpack, extend\n",
        "from backpack.custom_module.graph_utils import BackpackTracer\n",
        "\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "manual_seed(0)\n",
        "\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zidgMsibodl"
      },
      "source": [
        "### 2. Load data\n",
        "<a id='section_2'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, filename_csv, transform = None):\n",
        "        # read the csv file\n",
        "        self.df = pd.read_csv(filename_csv, sep=',')\n",
        "        # self.df = self.df.dropna(axis=0)\n",
        "        # save cols\n",
        "        self.input_cols = self.df['image'].tolist()\n",
        "        self.output_cols = self.df['class'].tolist()\n",
        "        self.transform = transform\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        # here i will return the number of samples in the dataset\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # here i will load the file in position idx\n",
        "        image_name = self.input_cols[idx]\n",
        "        # split in input / ground-truth\n",
        "        \n",
        "        image = np.array(Image.open(\"T:/CUB_dataset/images/\" + image_name))\n",
        "\n",
        "        augmented_images = self.transform(image=image)\n",
        "        image = augmented_images['image']\n",
        "\n",
        "        # return values\n",
        "        return image, self.output_cols[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform_train = A.Compose([\n",
        "  A.Resize(64,64),\n",
        "  A.HorizontalFlip(),\n",
        "  A.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406],               \n",
        "    std=[0.229, 0.224, 0.225]),\n",
        "  ToTensorV2()\n",
        "])\n",
        "\n",
        "transform_test = A.Compose([\n",
        "  A.Resize(64,64),\n",
        "  A.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406],               \n",
        "    std=[0.229, 0.224, 0.225]),\n",
        "  ToTensorV2()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create the training dataset\n",
        "\n",
        "train_ds = Dataset('T:/CUB_dataset/train.csv', transform_train)\n",
        "image, c = train_ds.__getitem__(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1007793..2.4831371].\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Showing an example\n",
        "\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the validation dataset\n",
        "\n",
        "val_ds = Dataset('T:/CUB_dataset/val.csv', transform_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK4uFm0mbodq"
      },
      "source": [
        "### 3. Load Model\n",
        "<a id='section_3'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test w/ Backpack to verify correctness of the converter + extension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "opcode       name                   target                 args                                   kwargs\n",
            "-----------  ---------------------  ---------------------  -------------------------------------  --------\n",
            "placeholder  x                      x                      ()                                     {}\n",
            "call_module  conv1                  conv1                  (x,)                                   {}\n",
            "call_module  bn1                    bn1                    (conv1,)                               {}\n",
            "call_module  relu                   relu                   (bn1,)                                 {}\n",
            "call_module  maxpool                maxpool                (relu,)                                {}\n",
            "call_module  layer1_0_conv1         layer1.0.conv1         (maxpool,)                             {}\n",
            "call_module  layer1_0_bn1           layer1.0.bn1           (layer1_0_conv1,)                      {}\n",
            "call_module  layer1_0_relu0         layer1.0.relu0         (layer1_0_bn1,)                        {}\n",
            "call_module  layer1_0_conv2         layer1.0.conv2         (layer1_0_relu0,)                      {}\n",
            "call_module  layer1_0_bn2           layer1.0.bn2           (layer1_0_conv2,)                      {}\n",
            "call_module  layer1_0_relu1         layer1.0.relu1         (layer1_0_bn2,)                        {}\n",
            "call_module  layer1_0_conv3         layer1.0.conv3         (layer1_0_relu1,)                      {}\n",
            "call_module  layer1_0_bn3           layer1.0.bn3           (layer1_0_conv3,)                      {}\n",
            "call_module  layer1_0_downsample_0  layer1.0.downsample.0  (maxpool,)                             {}\n",
            "call_module  layer1_0_downsample_1  layer1.0.downsample.1  (layer1_0_downsample_0,)               {}\n",
            "call_module  sum_module0            sum_module0            (layer1_0_bn3, layer1_0_downsample_1)  {}\n",
            "call_module  layer1_0_relu2         layer1.0.relu2         (sum_module0,)                         {}\n",
            "call_module  layer1_1_conv1         layer1.1.conv1         (layer1_0_relu2,)                      {}\n",
            "call_module  layer1_1_bn1           layer1.1.bn1           (layer1_1_conv1,)                      {}\n",
            "call_module  layer1_1_relu0         layer1.1.relu0         (layer1_1_bn1,)                        {}\n",
            "call_module  layer1_1_conv2         layer1.1.conv2         (layer1_1_relu0,)                      {}\n",
            "call_module  layer1_1_bn2           layer1.1.bn2           (layer1_1_conv2,)                      {}\n",
            "call_module  layer1_1_relu1         layer1.1.relu1         (layer1_1_bn2,)                        {}\n",
            "call_module  layer1_1_conv3         layer1.1.conv3         (layer1_1_relu1,)                      {}\n",
            "call_module  layer1_1_bn3           layer1.1.bn3           (layer1_1_conv3,)                      {}\n",
            "call_module  sum_module1            sum_module1            (layer1_1_bn3, layer1_0_relu2)         {}\n",
            "call_module  layer1_1_relu2         layer1.1.relu2         (sum_module1,)                         {}\n",
            "call_module  layer1_2_conv1         layer1.2.conv1         (layer1_1_relu2,)                      {}\n",
            "call_module  layer1_2_bn1           layer1.2.bn1           (layer1_2_conv1,)                      {}\n",
            "call_module  layer1_2_relu0         layer1.2.relu0         (layer1_2_bn1,)                        {}\n",
            "call_module  layer1_2_conv2         layer1.2.conv2         (layer1_2_relu0,)                      {}\n",
            "call_module  layer1_2_bn2           layer1.2.bn2           (layer1_2_conv2,)                      {}\n",
            "call_module  layer1_2_relu1         layer1.2.relu1         (layer1_2_bn2,)                        {}\n",
            "call_module  layer1_2_conv3         layer1.2.conv3         (layer1_2_relu1,)                      {}\n",
            "call_module  layer1_2_bn3           layer1.2.bn3           (layer1_2_conv3,)                      {}\n",
            "call_module  sum_module2            sum_module2            (layer1_2_bn3, layer1_1_relu2)         {}\n",
            "call_module  layer1_2_relu2         layer1.2.relu2         (sum_module2,)                         {}\n",
            "call_module  layer2_0_conv1         layer2.0.conv1         (layer1_2_relu2,)                      {}\n",
            "call_module  layer2_0_bn1           layer2.0.bn1           (layer2_0_conv1,)                      {}\n",
            "call_module  layer2_0_relu0         layer2.0.relu0         (layer2_0_bn1,)                        {}\n",
            "call_module  layer2_0_conv2         layer2.0.conv2         (layer2_0_relu0,)                      {}\n",
            "call_module  layer2_0_bn2           layer2.0.bn2           (layer2_0_conv2,)                      {}\n",
            "call_module  layer2_0_relu1         layer2.0.relu1         (layer2_0_bn2,)                        {}\n",
            "call_module  layer2_0_conv3         layer2.0.conv3         (layer2_0_relu1,)                      {}\n",
            "call_module  layer2_0_bn3           layer2.0.bn3           (layer2_0_conv3,)                      {}\n",
            "call_module  layer2_0_downsample_0  layer2.0.downsample.0  (layer1_2_relu2,)                      {}\n",
            "call_module  layer2_0_downsample_1  layer2.0.downsample.1  (layer2_0_downsample_0,)               {}\n",
            "call_module  sum_module3            sum_module3            (layer2_0_bn3, layer2_0_downsample_1)  {}\n",
            "call_module  layer2_0_relu2         layer2.0.relu2         (sum_module3,)                         {}\n",
            "call_module  layer2_1_conv1         layer2.1.conv1         (layer2_0_relu2,)                      {}\n",
            "call_module  layer2_1_bn1           layer2.1.bn1           (layer2_1_conv1,)                      {}\n",
            "call_module  layer2_1_relu0         layer2.1.relu0         (layer2_1_bn1,)                        {}\n",
            "call_module  layer2_1_conv2         layer2.1.conv2         (layer2_1_relu0,)                      {}\n",
            "call_module  layer2_1_bn2           layer2.1.bn2           (layer2_1_conv2,)                      {}\n",
            "call_module  layer2_1_relu1         layer2.1.relu1         (layer2_1_bn2,)                        {}\n",
            "call_module  layer2_1_conv3         layer2.1.conv3         (layer2_1_relu1,)                      {}\n",
            "call_module  layer2_1_bn3           layer2.1.bn3           (layer2_1_conv3,)                      {}\n",
            "call_module  sum_module4            sum_module4            (layer2_1_bn3, layer2_0_relu2)         {}\n",
            "call_module  layer2_1_relu2         layer2.1.relu2         (sum_module4,)                         {}\n",
            "call_module  layer2_2_conv1         layer2.2.conv1         (layer2_1_relu2,)                      {}\n",
            "call_module  layer2_2_bn1           layer2.2.bn1           (layer2_2_conv1,)                      {}\n",
            "call_module  layer2_2_relu0         layer2.2.relu0         (layer2_2_bn1,)                        {}\n",
            "call_module  layer2_2_conv2         layer2.2.conv2         (layer2_2_relu0,)                      {}\n",
            "call_module  layer2_2_bn2           layer2.2.bn2           (layer2_2_conv2,)                      {}\n",
            "call_module  layer2_2_relu1         layer2.2.relu1         (layer2_2_bn2,)                        {}\n",
            "call_module  layer2_2_conv3         layer2.2.conv3         (layer2_2_relu1,)                      {}\n",
            "call_module  layer2_2_bn3           layer2.2.bn3           (layer2_2_conv3,)                      {}\n",
            "call_module  sum_module5            sum_module5            (layer2_2_bn3, layer2_1_relu2)         {}\n",
            "call_module  layer2_2_relu2         layer2.2.relu2         (sum_module5,)                         {}\n",
            "call_module  layer2_3_conv1         layer2.3.conv1         (layer2_2_relu2,)                      {}\n",
            "call_module  layer2_3_bn1           layer2.3.bn1           (layer2_3_conv1,)                      {}\n",
            "call_module  layer2_3_relu0         layer2.3.relu0         (layer2_3_bn1,)                        {}\n",
            "call_module  layer2_3_conv2         layer2.3.conv2         (layer2_3_relu0,)                      {}\n",
            "call_module  layer2_3_bn2           layer2.3.bn2           (layer2_3_conv2,)                      {}\n",
            "call_module  layer2_3_relu1         layer2.3.relu1         (layer2_3_bn2,)                        {}\n",
            "call_module  layer2_3_conv3         layer2.3.conv3         (layer2_3_relu1,)                      {}\n",
            "call_module  layer2_3_bn3           layer2.3.bn3           (layer2_3_conv3,)                      {}\n",
            "call_module  sum_module6            sum_module6            (layer2_3_bn3, layer2_2_relu2)         {}\n",
            "call_module  layer2_3_relu2         layer2.3.relu2         (sum_module6,)                         {}\n",
            "call_module  layer3_0_conv1         layer3.0.conv1         (layer2_3_relu2,)                      {}\n",
            "call_module  layer3_0_bn1           layer3.0.bn1           (layer3_0_conv1,)                      {}\n",
            "call_module  layer3_0_relu0         layer3.0.relu0         (layer3_0_bn1,)                        {}\n",
            "call_module  layer3_0_conv2         layer3.0.conv2         (layer3_0_relu0,)                      {}\n",
            "call_module  layer3_0_bn2           layer3.0.bn2           (layer3_0_conv2,)                      {}\n",
            "call_module  layer3_0_relu1         layer3.0.relu1         (layer3_0_bn2,)                        {}\n",
            "call_module  layer3_0_conv3         layer3.0.conv3         (layer3_0_relu1,)                      {}\n",
            "call_module  layer3_0_bn3           layer3.0.bn3           (layer3_0_conv3,)                      {}\n",
            "call_module  layer3_0_downsample_0  layer3.0.downsample.0  (layer2_3_relu2,)                      {}\n",
            "call_module  layer3_0_downsample_1  layer3.0.downsample.1  (layer3_0_downsample_0,)               {}\n",
            "call_module  sum_module7            sum_module7            (layer3_0_bn3, layer3_0_downsample_1)  {}\n",
            "call_module  layer3_0_relu2         layer3.0.relu2         (sum_module7,)                         {}\n",
            "call_module  layer3_1_conv1         layer3.1.conv1         (layer3_0_relu2,)                      {}\n",
            "call_module  layer3_1_bn1           layer3.1.bn1           (layer3_1_conv1,)                      {}\n",
            "call_module  layer3_1_relu0         layer3.1.relu0         (layer3_1_bn1,)                        {}\n",
            "call_module  layer3_1_conv2         layer3.1.conv2         (layer3_1_relu0,)                      {}\n",
            "call_module  layer3_1_bn2           layer3.1.bn2           (layer3_1_conv2,)                      {}\n",
            "call_module  layer3_1_relu1         layer3.1.relu1         (layer3_1_bn2,)                        {}\n",
            "call_module  layer3_1_conv3         layer3.1.conv3         (layer3_1_relu1,)                      {}\n",
            "call_module  layer3_1_bn3           layer3.1.bn3           (layer3_1_conv3,)                      {}\n",
            "call_module  sum_module8            sum_module8            (layer3_1_bn3, layer3_0_relu2)         {}\n",
            "call_module  layer3_1_relu2         layer3.1.relu2         (sum_module8,)                         {}\n",
            "call_module  layer3_2_conv1         layer3.2.conv1         (layer3_1_relu2,)                      {}\n",
            "call_module  layer3_2_bn1           layer3.2.bn1           (layer3_2_conv1,)                      {}\n",
            "call_module  layer3_2_relu0         layer3.2.relu0         (layer3_2_bn1,)                        {}\n",
            "call_module  layer3_2_conv2         layer3.2.conv2         (layer3_2_relu0,)                      {}\n",
            "call_module  layer3_2_bn2           layer3.2.bn2           (layer3_2_conv2,)                      {}\n",
            "call_module  layer3_2_relu1         layer3.2.relu1         (layer3_2_bn2,)                        {}\n",
            "call_module  layer3_2_conv3         layer3.2.conv3         (layer3_2_relu1,)                      {}\n",
            "call_module  layer3_2_bn3           layer3.2.bn3           (layer3_2_conv3,)                      {}\n",
            "call_module  sum_module9            sum_module9            (layer3_2_bn3, layer3_1_relu2)         {}\n",
            "call_module  layer3_2_relu2         layer3.2.relu2         (sum_module9,)                         {}\n",
            "call_module  layer3_3_conv1         layer3.3.conv1         (layer3_2_relu2,)                      {}\n",
            "call_module  layer3_3_bn1           layer3.3.bn1           (layer3_3_conv1,)                      {}\n",
            "call_module  layer3_3_relu0         layer3.3.relu0         (layer3_3_bn1,)                        {}\n",
            "call_module  layer3_3_conv2         layer3.3.conv2         (layer3_3_relu0,)                      {}\n",
            "call_module  layer3_3_bn2           layer3.3.bn2           (layer3_3_conv2,)                      {}\n",
            "call_module  layer3_3_relu1         layer3.3.relu1         (layer3_3_bn2,)                        {}\n",
            "call_module  layer3_3_conv3         layer3.3.conv3         (layer3_3_relu1,)                      {}\n",
            "call_module  layer3_3_bn3           layer3.3.bn3           (layer3_3_conv3,)                      {}\n",
            "call_module  sum_module10           sum_module10           (layer3_3_bn3, layer3_2_relu2)         {}\n",
            "call_module  layer3_3_relu2         layer3.3.relu2         (sum_module10,)                        {}\n",
            "call_module  layer3_4_conv1         layer3.4.conv1         (layer3_3_relu2,)                      {}\n",
            "call_module  layer3_4_bn1           layer3.4.bn1           (layer3_4_conv1,)                      {}\n",
            "call_module  layer3_4_relu0         layer3.4.relu0         (layer3_4_bn1,)                        {}\n",
            "call_module  layer3_4_conv2         layer3.4.conv2         (layer3_4_relu0,)                      {}\n",
            "call_module  layer3_4_bn2           layer3.4.bn2           (layer3_4_conv2,)                      {}\n",
            "call_module  layer3_4_relu1         layer3.4.relu1         (layer3_4_bn2,)                        {}\n",
            "call_module  layer3_4_conv3         layer3.4.conv3         (layer3_4_relu1,)                      {}\n",
            "call_module  layer3_4_bn3           layer3.4.bn3           (layer3_4_conv3,)                      {}\n",
            "call_module  sum_module11           sum_module11           (layer3_4_bn3, layer3_3_relu2)         {}\n",
            "call_module  layer3_4_relu2         layer3.4.relu2         (sum_module11,)                        {}\n",
            "call_module  layer3_5_conv1         layer3.5.conv1         (layer3_4_relu2,)                      {}\n",
            "call_module  layer3_5_bn1           layer3.5.bn1           (layer3_5_conv1,)                      {}\n",
            "call_module  layer3_5_relu0         layer3.5.relu0         (layer3_5_bn1,)                        {}\n",
            "call_module  layer3_5_conv2         layer3.5.conv2         (layer3_5_relu0,)                      {}\n",
            "call_module  layer3_5_bn2           layer3.5.bn2           (layer3_5_conv2,)                      {}\n",
            "call_module  layer3_5_relu1         layer3.5.relu1         (layer3_5_bn2,)                        {}\n",
            "call_module  layer3_5_conv3         layer3.5.conv3         (layer3_5_relu1,)                      {}\n",
            "call_module  layer3_5_bn3           layer3.5.bn3           (layer3_5_conv3,)                      {}\n",
            "call_module  sum_module12           sum_module12           (layer3_5_bn3, layer3_4_relu2)         {}\n",
            "call_module  layer3_5_relu2         layer3.5.relu2         (sum_module12,)                        {}\n",
            "call_module  layer4_0_conv1         layer4.0.conv1         (layer3_5_relu2,)                      {}\n",
            "call_module  layer4_0_bn1           layer4.0.bn1           (layer4_0_conv1,)                      {}\n",
            "call_module  layer4_0_relu0         layer4.0.relu0         (layer4_0_bn1,)                        {}\n",
            "call_module  layer4_0_conv2         layer4.0.conv2         (layer4_0_relu0,)                      {}\n",
            "call_module  layer4_0_bn2           layer4.0.bn2           (layer4_0_conv2,)                      {}\n",
            "call_module  layer4_0_relu1         layer4.0.relu1         (layer4_0_bn2,)                        {}\n",
            "call_module  layer4_0_conv3         layer4.0.conv3         (layer4_0_relu1,)                      {}\n",
            "call_module  layer4_0_bn3           layer4.0.bn3           (layer4_0_conv3,)                      {}\n",
            "call_module  layer4_0_downsample_0  layer4.0.downsample.0  (layer3_5_relu2,)                      {}\n",
            "call_module  layer4_0_downsample_1  layer4.0.downsample.1  (layer4_0_downsample_0,)               {}\n",
            "call_module  sum_module13           sum_module13           (layer4_0_bn3, layer4_0_downsample_1)  {}\n",
            "call_module  layer4_0_relu2         layer4.0.relu2         (sum_module13,)                        {}\n",
            "call_module  layer4_1_conv1         layer4.1.conv1         (layer4_0_relu2,)                      {}\n",
            "call_module  layer4_1_bn1           layer4.1.bn1           (layer4_1_conv1,)                      {}\n",
            "call_module  layer4_1_relu0         layer4.1.relu0         (layer4_1_bn1,)                        {}\n",
            "call_module  layer4_1_conv2         layer4.1.conv2         (layer4_1_relu0,)                      {}\n",
            "call_module  layer4_1_bn2           layer4.1.bn2           (layer4_1_conv2,)                      {}\n",
            "call_module  layer4_1_relu1         layer4.1.relu1         (layer4_1_bn2,)                        {}\n",
            "call_module  layer4_1_conv3         layer4.1.conv3         (layer4_1_relu1,)                      {}\n",
            "call_module  layer4_1_bn3           layer4.1.bn3           (layer4_1_conv3,)                      {}\n",
            "call_module  sum_module14           sum_module14           (layer4_1_bn3, layer4_0_relu2)         {}\n",
            "call_module  layer4_1_relu2         layer4.1.relu2         (sum_module14,)                        {}\n",
            "call_module  layer4_2_conv1         layer4.2.conv1         (layer4_1_relu2,)                      {}\n",
            "call_module  layer4_2_bn1           layer4.2.bn1           (layer4_2_conv1,)                      {}\n",
            "call_module  layer4_2_relu0         layer4.2.relu0         (layer4_2_bn1,)                        {}\n",
            "call_module  layer4_2_conv2         layer4.2.conv2         (layer4_2_relu0,)                      {}\n",
            "call_module  layer4_2_bn2           layer4.2.bn2           (layer4_2_conv2,)                      {}\n",
            "call_module  layer4_2_relu1         layer4.2.relu1         (layer4_2_bn2,)                        {}\n",
            "call_module  layer4_2_conv3         layer4.2.conv3         (layer4_2_relu1,)                      {}\n",
            "call_module  layer4_2_bn3           layer4.2.bn3           (layer4_2_conv3,)                      {}\n",
            "call_module  sum_module15           sum_module15           (layer4_2_bn3, layer4_1_relu2)         {}\n",
            "call_module  layer4_2_relu2         layer4.2.relu2         (sum_module15,)                        {}\n",
            "call_module  avgpool                avgpool                (layer4_2_relu2,)                      {}\n",
            "call_module  flatten0               flatten0               (avgpool,)                             {}\n",
            "call_module  fc                     fc                     (flatten0,)                            {}\n",
            "output       output                 output                 (fc,)                                  {}\n"
          ]
        }
      ],
      "source": [
        "DEVICE = \"cuda\"\n",
        "\n",
        "loss_function = extend(MSELoss().to(DEVICE))\n",
        "model = resnet50(num_classes=200).to(DEVICE).eval()\n",
        "\n",
        "model = extend(model, use_converter=True)\n",
        "\n",
        "def print_table(module: Module) -> None:\n",
        "    \"\"\"Prints a table of the module.\n",
        "\n",
        "    Args:\n",
        "        module: module to analyze\n",
        "    \"\"\"\n",
        "    graph = BackpackTracer().trace(module)\n",
        "    graph.print_tabular()\n",
        "\n",
        "\n",
        "print_table(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=200, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = resnet50(num_classes=200).to(DEVICE).train()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training (hoping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_dataset(inp_file, batch_size=100, seq_length=50):\n",
        "    # Load the data from file\n",
        "    arr = np.load(inp_file)\n",
        "\n",
        "    # Determine how many full batches we can make\n",
        "    num_batches = (arr.size - 1) // (batch_size * seq_length)\n",
        "    if num_batches == 0:\n",
        "        raise ValueError(\n",
        "            \"This dataset is too small to use with \"\n",
        "            f\"batch_size={batch_size} and seq_length={seq_length}.\"\n",
        "        )\n",
        "\n",
        "    # Truncate array to fit exactly into num_batches\n",
        "    x = arr[:num_batches * batch_size * seq_length]\n",
        "    y = arr[1:num_batches * batch_size * seq_length + 1]\n",
        "\n",
        "    # Reshape so each row is one sequence\n",
        "    X = x.reshape(-1, seq_length)\n",
        "    Y = y.reshape(-1, seq_length)\n",
        "\n",
        "    # Create a TensorDataset and wrap it in a DataLoader\n",
        "    dataset = TensorDataset(\n",
        "        torch.Tensor(X).long(),\n",
        "        torch.Tensor(Y).long()\n",
        "    )\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    \n",
        "    return dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch index: 0\n",
            "X_batch shape: torch.Size([100, 50])\n",
            "Y_batch shape: torch.Size([100, 50])\n"
          ]
        }
      ],
      "source": [
        "train_filepath = \"C:/Users/user/Desktop/PhD/Paper - Manini/FisherUnlearning/data/train.npy\"\n",
        "test_filepath = \"C:/Users/user/Desktop/PhD/Paper - Manini/FisherUnlearning/data/test.npy\"\n",
        "\n",
        "batch_size = 100\n",
        "seq_length = 50\n",
        "\n",
        "train_dataloader = prepare_dataset(train_filepath, batch_size, seq_length)\n",
        "test_dataloader = prepare_dataset(test_filepath, batch_size, seq_length)\n",
        "\n",
        "for i, (X_batch, Y_batch) in enumerate(train_dataloader):\n",
        "    print(\"Batch index:\", i)\n",
        "    print(\"X_batch shape:\", X_batch.shape)\n",
        "    print(\"Y_batch shape:\", Y_batch.shape)\n",
        "    break  # Stop after the first batch\n",
        "\n",
        "#TrainLoader has 512 Batches\n",
        "#TestLoader has 128 Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/1000 [00:17<4:53:57, 17.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 0: Cross-entropy: 339.9692\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 2/1000 [00:35<4:51:06, 17.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 1: Cross-entropy: 339.6379\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 3/1000 [00:52<4:48:20, 17.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 2: Cross-entropy: 339.6246\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 4/1000 [01:09<4:46:39, 17.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 3: Cross-entropy: 339.5884\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 5/1000 [01:26<4:46:10, 17.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 4: Cross-entropy: 339.4312\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 6/1000 [01:43<4:46:02, 17.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Cross-entropy: 339.6255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 7/1000 [02:01<4:45:09, 17.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Cross-entropy: 339.4424\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 8/1000 [02:18<4:44:18, 17.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Cross-entropy: 339.5302\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 9/1000 [02:35<4:43:42, 17.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Cross-entropy: 339.4560\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 10/1000 [02:52<4:43:09, 17.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Cross-entropy: 339.4649\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 11/1000 [03:09<4:42:54, 17.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Cross-entropy: 339.4488\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 12/1000 [03:26<4:42:24, 17.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Cross-entropy: 339.4341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|         | 13/1000 [03:43<4:42:27, 17.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 12: Cross-entropy: 339.4211\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|         | 14/1000 [04:01<4:42:19, 17.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 13: Cross-entropy: 339.4084\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|         | 15/1000 [04:18<4:42:12, 17.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 14: Cross-entropy: 339.3970\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|         | 16/1000 [04:35<4:41:58, 17.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 15: Cross-entropy: 339.3863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|         | 17/1000 [04:52<4:41:56, 17.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 16: Cross-entropy: 339.3762\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|         | 18/1000 [05:10<4:41:44, 17.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 17: Cross-entropy: 339.3664\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|         | 19/1000 [05:27<4:41:25, 17.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 18: Cross-entropy: 339.3574\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|         | 20/1000 [05:44<4:41:08, 17.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 19: Cross-entropy: 339.3488\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|         | 21/1000 [06:01<4:40:50, 17.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 20: Cross-entropy: 339.3403\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|         | 22/1000 [06:18<4:40:37, 17.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 21: Cross-entropy: 339.3325\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|         | 23/1000 [06:36<4:40:29, 17.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 22: Cross-entropy: 339.3251\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|         | 24/1000 [06:53<4:40:14, 17.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 23: Cross-entropy: 339.3180\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|         | 25/1000 [07:10<4:39:59, 17.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 24: Cross-entropy: 339.3111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|         | 26/1000 [07:27<4:39:47, 17.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 25: Cross-entropy: 339.3047\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|         | 27/1000 [07:45<4:39:33, 17.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 26: Cross-entropy: 339.2988\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|         | 28/1000 [08:02<4:39:08, 17.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 27: Cross-entropy: 339.2931\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|         | 29/1000 [08:19<4:38:54, 17.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 28: Cross-entropy: 339.2883\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|         | 30/1000 [08:36<4:38:42, 17.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**BEST**Epoch 29: Cross-entropy: 339.2882\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|         | 31/1000 [08:54<4:38:18, 17.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Cross-entropy: 339.2883\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|         | 32/1000 [09:11<4:37:52, 17.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31: Cross-entropy: 339.2883\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|         | 33/1000 [09:28<4:37:37, 17.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32: Cross-entropy: 339.2883\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|         | 34/1000 [09:45<4:37:17, 17.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33: Cross-entropy: 339.2882\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|         | 35/1000 [10:02<4:37:04, 17.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34: Cross-entropy: 339.2882\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|         | 36/1000 [10:20<4:37:30, 17.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35: Cross-entropy: 339.2883\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|         | 37/1000 [10:37<4:37:23, 17.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36: Cross-entropy: 339.2882\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|         | 38/1000 [10:55<4:36:27, 17.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37: Cross-entropy: 339.2882\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[36], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 27\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m tolstoi_char_rnn_custom(X_batch\u001b[38;5;241m.\u001b[39mto(DEVICE))\n\u001b[0;32m     28\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(y_pred, y_batch\u001b[38;5;241m.\u001b[39mto(DEVICE))\n\u001b[0;32m     29\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.SGD(tolstoi_char_rnn_custom.parameters(), lr=0.04)\n",
        "optimizer = optim.Adam(tolstoi_char_rnn_custom.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# Create a scheduler, e.g., ReduceLROnPlateau\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.0001,    # how much to reduce the LR by\n",
        "    patience=4,    # how many epochs to wait before reducing LR\n",
        "    verbose=True   # prints a message each time LR is reduced\n",
        ")\n",
        "\n",
        "n_epochs = 1000\n",
        "best_model = None\n",
        "best_loss = np.inf\n",
        "patience = 8\n",
        "\n",
        "for epoch in tqdm(range(n_epochs)):\n",
        "    ###################\n",
        "    # Training phase\n",
        "    ###################\n",
        "    tolstoi_char_rnn_custom.train()\n",
        "    for X_batch, y_batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = tolstoi_char_rnn_custom(X_batch.to(DEVICE))\n",
        "        loss = loss_function(y_pred, y_batch.to(DEVICE))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    ###################\n",
        "    # Validation phase\n",
        "    ###################\n",
        "    tolstoi_char_rnn_custom.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_dataloader:\n",
        "            y_pred = tolstoi_char_rnn_custom(X_batch.to(DEVICE))\n",
        "            val_loss += loss_function(y_pred, y_batch.to(DEVICE))\n",
        "   \n",
        "    # Step the scheduler using the validation loss\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Check for best model & early stopping\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_model = tolstoi_char_rnn_custom.state_dict()\n",
        "        torch.save(best_model, \"single-char.pth\")\n",
        "        patience = 20\n",
        "        print(\"**BEST**\", end=\"\")\n",
        "    else:\n",
        "        patience -= 1\n",
        "\n",
        "    print(f\"Epoch {epoch}: Cross-entropy: {val_loss:.4f}\")\n",
        "\n",
        "    if patience <= 0:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([100, 28, 100])\n",
            "torch.Size([100, 100])\n",
            "Predicted: e th n     ete  e  an n    a        hon     hn     aan  aen tan  aa   aeeeehhe  he te  aa   tnto neh\n",
            "True:      e princess. he had changed verymuch since princess mary had last seen him. then he had been a briskc\n",
            "----------\n",
            "Predicted: h       t           a   aondho  he th     tnahn nn  to                ta    th   n  th hh e     t n \n",
            "True:      heerful selfassured old man now he seemed a pitiful bewilderedperson. while talking to princess mary\n",
            "----------\n",
            "Predicted:  he te   ng     a            hn n tn tn  t        t e     he t n aa    ahe ten  eahen   tn    ah    \n",
            "True:       he continually looked round asif asking everyone whether he was doing the right thing. after thedes\n",
            "----------\n",
            "Predicted:       n  t  to     and a  ten hh       ahee   t   h  aen an    h e           t      ah hen  t    hhe\n",
            "True:      truction of moscow and of his property thrown out of his accustomedgroove he seemed to have lost the\n",
            "----------\n",
            "Predicted:   e    a  ten h o aae  n n   n  n  ah     the  ahe   t n to a      tnt  n  te  ten tngatnn  n t  n  \n",
            "True:       sense of his own significance and tofeel that there was no longer a place for him in life.in spite \n",
            "----------\n",
            "Predicted: aoote  a   ta  e  ah ao  te eaee  e  anaha   tn ao   n   tn     te  nen  hh   hnhhhe ta  r  hae  tn \n",
            "True:      of her one desire to see her brother as soon as possible andher vexation that at the moment when all\n",
            "----------\n",
            "Predicted: lae  ahn    a n hh t   tennahe        te th en  th h  e    ngte  and a h     n  th a   n  he  ao    \n",
            "True:       she wanted was to see him theyshould be trying to entertain her and pretending to admire her nephew\n",
            "----------\n",
            "Predicted: ahe t e      th  n   a   the  to  he n  t  tn     ae  a   a    ahe       n  t  t    n  n  to  tnthe \n",
            "True:      the princess noticed all that was going on around her and felt thenecessity of submitting for a time\n",
            "----------\n",
            "Predicted:     hee  he  ae    aeethee   aaeneeee te  a e      toe th   tnhah ho t         a     e    aehao  te \n",
            "True:       to this new order of things whichshe had entered. she knew it to be necessary and though it was har\n",
            "----------\n",
            "Predicted:   aee e  aee ten a   h     h nh the   t           anhha h e   t  n ahe       tng h    n  th      to \n",
            "True:      d forher she was not vexed with these people.this is my niece said the count introducing snyayou don\n",
            "----------\n",
            "Predicted:   aoe  e  ta n             ten eah     ah hh  nt   ahee   hh hohnne hhe te          n  the  an    hn\n",
            "True:      t knowher princessprincess mary turned to snya and trying to stifle the hostilefeeling that arose in\n",
            "----------\n",
            "Predicted:  ae  ah  n  ahe  on  ao  ton    ae e ta  t e te            te ahe  e   ahe  ah  ta   a  aa       tn \n",
            "True:       her toward the girl she kissed her. but she feltoppressed by the fact that the mood of everyone aro\n",
            "----------\n",
            "Predicted: n   ae  ta  th aon     ao   hen aegte  h   he      e   tn te a e tn t  a   ngan      n  ahe  hnn    \n",
            "True:      und her was so farfrom what was in her own heart.where is he she asked again addressing them all.he \n",
            "----------\n",
            "Predicted: anhht  e  nn  hth h e tt hhnhetenetn       a e  tee  eng  ta e   to   ah tn hetnthee  ho  to  hae th\n",
            "True:      is downstairs. natsha is with him answered snya flushing. wehave sent to ask. i think you must be ti\n",
            "----------\n",
            "Predicted: nt  t ee        n  a  t   n n  h      ahe        an he       a n   a     t e th e       tnd t   an  \n",
            "True:      red princess.tears of vexation showed themselves in princess marys eyes. she turnedaway and was abou\n",
            "----------\n",
            "Predicted: t th aeeathe ta  h    an  n tee th  h hheheneaae   n e an        ht  ah   n    he      t     he   te\n",
            "True:      t to ask the countess again how to go to him whenlight impetuous and seemingly buoyant steps were he\n",
            "----------\n",
            "Predicted:     an hhe h     h  a hn     te     ah    an  hh  a      te  n  tngan     hh    g           aee  the\n",
            "True:      ard at the door.the princess looked round and saw natsha coming in almost runningthatnatsha whom she\n",
            "----------\n",
            "Predicted:   en hen   a  aenhh  t  hhe n ao      te ho     h     ng      ae     ae  hhe    n     t   t  an ah  \n",
            "True:       had liked so little at their meeting in moscow longsince.but hardly had the princess looked at nats\n",
            "----------\n",
            "Predicted:  e  hen  te     toe te           ae e ten tnah   a    n  hn ae  he n  hn  ae           anto n       \n",
            "True:      has face before she realizedthat here was a real comrade in her grief and consequently a friend.she \n",
            "----------\n",
            "Predicted: aon hh ho  hhe  a a  n   ae  an  aa    th toe ae te  aoee        ah   tn ao   e thn hn  an ahe te   \n",
            "True:      ran to meet her embraced her and began to cry on her shoulder.as soon as natsha sitting at the head \n",
            "----------\n",
            "Predicted: aooto n   tn      te  ae n    to n     t n e tn  n n aa  tae    a    hen th   tn  aen h    ah ae    \n",
            "True:      of prince andrews bed heardof princess marys arrival she softly left his room and hastened to herwit\n",
            "----------\n",
            "Predicted: hetoee  aaen haeh   ahe eaen ah      ao   n  ah t  n     aen   he   te  a    a   t       n  t  te  a\n",
            "True:      h those swift steps that had sounded buoyant to princess mary.there was only one expression on her a\n",
            "----------\n",
            "Predicted: n n  e  he   the  the th  tn   hhe     n  th   e   h  h                  ao  ten te  he  hnd ae  tn \n",
            "True:      gitated face when she ran into thedrawing roomthat of loveboundless love for him for her and for all\n",
            "----------\n",
            "Predicted: l en aen ah   th ahe tan hhe a     an  a  a nh ao      g ta  te e   n  a n an  n  t   n  ah hhn  te \n",
            "True:      that was near to the man she loved and of pity suffering for othersand passionate desire to give her\n",
            "----------\n",
            "Predicted:  e   t  hn    hh ha   n  the   hn ahn   nngthe  anhahe hhhee  hhhe   t   hngto   e  ae    ta hhe    \n",
            "True:      self entirely to helping them. it wasplain that at that moment there was in natshas heart no thought\n",
            "----------\n",
            "Predicted: haoee e    a  to ta  aee t   t nn  tanheth n   t  a      n     aen taon ete ean    ao   n nnn  th   \n",
            "True:       ofherself or of her own relations with prince andrew.princess mary with her acute sensibility under\n",
            "----------\n",
            "Predicted:  eh   ae  hheeht  ahe  n   aae d  tn aon  e  aon  tnd ta   ho ae  hhen     aan eah  e               \n",
            "True:      stood all this at thefirst glance at natshas face and wept on her shoulder with sorrowfulpleasure.co\n",
            "----------\n",
            "Predicted: n  t n  th he  hen  ae n t      ae   n  te  an   the t  e         n     a    ah ne  ae  h     o n  h\n",
            "True:      me come to him mary said natsha leading her into the otherroom.princess mary raised her head dried h\n",
            "----------\n",
            "Predicted: e       an  ah     ah a    e     thr  hhe hhee  ae  aee ta    aa tnn  hh ho         an  a           \n",
            "True:      er eyes and turned to natsha.she felt that from her she would be able to understand and learneveryth\n",
            "----------\n",
            "Predicted: eng            tor   ae  a e  en  ao  ah      a e       to   hhe  an thn anth h n n ah hn aa ehhehn \n",
            "True:      ing.how... she began her question but stopped short.she felt that it was impossible to ask or to ans\n",
            "----------\n",
            "Predicted:     te he  e      e  ton  tn  a    ha    aen  th ah  tte ean nto   he  n   an     e             t n \n",
            "True:      wer in words.natshas face and eyes would have to tell her all more clearly andprofoundly.natsha was \n",
            "----------\n",
            "Predicted: ahn ng tn he  aa  ao     tn   n tn  antao    aee  e  ah aon nn a e tah  t  th  the ta     ah hor  ah\n",
            "True:      gazing at her but seemed afraid and in doubt whether to sayall she knew or not she seemed to feel th\n",
            "----------\n",
            "Predicted: et th     ahe a ta  n    a      n  a      n   an h ahe a  e ta     a  ta  ae    hohaan an   h nen   \n",
            "True:      at before those luminous eyeswhich penetrated into the very depths of her heart it was impossiblenot\n",
            "----------\n",
            "Predicted: hah ah   ahe ta    th   eaaen ea e ah   tnd aa       a       hen  h nh e  ao   haen     t        a  \n",
            "True:       to tell the whole truth which she saw. and suddenly natshas lipstwitched ugly wrinkles gathered rou\n",
            "----------\n",
            "Predicted: t  ee  at  eeand he       te  at    n  te  aa d  a   tod   an   th      ng    t   eand            a \n",
            "True:      nd her mouth and covering her facewith her hands she burst into sobs.princess mary understood.but sh\n",
            "----------\n",
            "Predicted: e te n nte    an  a  e  tetaa    a e te      ton t   ah       he  hn heneto    aaen an  an ah t  n a\n",
            "True:      e still hoped and asked in words she herself did not trustbut how is his wound what is his general c\n",
            "----------\n",
            "Predicted: hn  n n g          ten  t   a   an  th   e te    a        ahn anttn h  th    h       n  a    ten h  \n",
            "True:      onditionyou you... will see was all natsha could say.they sat a little while downstairs near his roo\n",
            "----------\n",
            "Predicted: n th   t e ete  he   a         tn  aa e a    thea  th aeenhaneete e ahn       te  aeneahe   tn n    \n",
            "True:      m till they had left offcrying and were able to go to him with calm faces.how has his whole illness \n",
            "----------\n",
            "Predicted: ahn han tn te   ton   te t    ta      e   n hh n ten    t  n     tan  an t t          ah e ae  ahe  \n",
            "True:      gone is it long since he grew worse whendid this happen princess mary inquired.natsha told her that \n",
            "----------\n",
            "Predicted: anhhone  aoe   te  te   ta     aoe  aen ah    n e    nhn  tnd a e t  e ae tat      aa  a  ah n  nahe\n",
            "True:      at first there had been danger from his feverishcondition and the pain he suffered but at tritsa tha\n",
            "----------\n",
            "Predicted: nhaeneton     d the a      te  h    te   tne dn t  aond      the hahn    aan aneh  n     tee  the  h\n",
            "True:      t had passedand the doctor had only been afraid of gangrene. that danger had alsopassed. when they r\n",
            "----------\n",
            "Predicted:     e  aen      hhe te    he  ae    hh hh           hhh  hn etn    thh  th n   an th     nh tnd ahe \n",
            "True:      eached yaroslvl the wound had begun to festernatsha knew all about such things as festering and the \n",
            "----------\n",
            "Predicted: aoe    ae    n hhe hhhe to      g aan ehahnh tnhh   n an      the  to    ho  ann   ahe t      ae  h \n",
            "True:      doctor hadsaid that the festering might take a normal course. then fever set inbut the doctor had sa\n",
            "----------\n",
            "Predicted: nn ah  aeee  ae  ao  ao   a   n       ahe hhn ean  then hh       aen       e neh n    thh     n  tee\n",
            "True:      id the fever was not very serious.but two days ago this suddenly happened said natsha struggling wit\n",
            "----------\n",
            "Predicted: hee  ah    hetae  aah  ahe ao haee aanetae  aae hae tt thnt  neta aa     ahe     an ar ahe t  n     \n",
            "True:      hher sobs. i dont know why but you will see what he is like.is he weaker thinner asked the princess.\n",
            "----------\n",
            "Predicted:   u nh he  ahe  ae  ae     ah  aon  te   t a neeae tn hh  ah   ao       he     tnn  ao              \n",
            "True:      no its not that but worse. you will see. o mary he is too good hecannot cannot live because...chapte\n",
            "----------\n",
            "Predicted:   taaee  t n  e a      a  n   tn      a    t n eanaon ntnn ae       ht     a  n     he  eahn  tn   a\n",
            "True:      r xvwhen natsha opened prince andrews door with a familiar movement andlet princess mary pass into t\n",
            "----------\n",
            "Predicted: he ee   ao      e  ahe    n     a    ahe     antae  ahee    ae   an ta  te  hh n  ah h       te     \n",
            "True:      he room before her the princess felt thesobs in her throat. hard as she had tried to prepare herself\n",
            "----------\n",
            "Predicted:  ane aoeah e ehh to  nngahhnd nntthe tah  ahe  hhe ta    ho a      th ha   hnhen aen e   ah  e ehe t\n",
            "True:       and nowtried to remain tranquil she knew that she would be unable to look athim without tears.the p\n",
            "----------\n",
            "Predicted:         a          aae  th   e ae  ao    ao ah  to    ah  th      the  thh    e he        ah  t     \n",
            "True:      rincess understood what natsha had meant by the words two daysago this suddenly happened. she unders\n",
            "----------\n",
            "Predicted:      hheee aa    ah ho   ahe  he e  a e      a        h   ahe  hhee hh     ng tnd ho         aa   t \n",
            "True:      tood those words to mean that hehad suddenly softened and that this softening and gentleness were si\n",
            "----------\n",
            "Predicted: n g   t         g aa     tn the t h     th hhe ta   hhe tn e    a   antnen n n n  tn      hon  tn t \n",
            "True:      gnsof approaching death. as she stepped to the door she already saw inimagination andrews face as sh\n",
            "----------\n",
            "Predicted: e t           n tn te n      tnte      n  t           t    t en  ae te  aet    a e   tn  t en  the  \n",
            "True:      e remembered it in childhood a gentlemild sympathetic face which he had rarely shown and which there\n",
            "----------\n",
            "Predicted:      n        e  h              t   aa  ah   ae ae    h     h    ah           e te  ao   a  he  ho  \n",
            "True:      foreaffected her very strongly. she was sure he would speak soft tenderwords to her such as her fath\n",
            "----------\n",
            "Predicted: e      a       a      aen ah   etn  h   ahe t     to  ho t  n th hh   an an  aa    a     ant  to   t\n",
            "True:      er had uttered before his death andthat she would not be able to bear it and would burst into sobs i\n",
            "----------\n",
            "Predicted: ngten          te  ah     a  ae   eanhae  ah h   n  a e t    ant the t        hen e  and aen     n a\n",
            "True:      n hispresence. yet sooner or later it had to be and she went in. the sobsrose higher and higher in h\n",
            "----------\n",
            "Predicted: e   he    aneahe ta    n  aa   te     e n  ng       aen ao   an  ae  aoe    n e   a e  ah n  hh hant\n",
            "True:      er throat as she more and more clearlydistinguished his form and her shortsighted eyes tried to make\n",
            "----------\n",
            "Predicted:      aene        tn  ah   th  tan hen aon  tnd ae  aee hh    e t   a   g todantho n       h        a\n",
            "True:       out hisfeatures and then she saw his face and met his gaze.he was lying in a squirrelfur dressing g\n",
            "----------\n",
            "Predicted:     t  ant n n h          a   n     nte a n ahen tn  a n   tntae  then th            aaen  hen  e te\n",
            "True:      own on a divan surrounded bypillows. he was thin and pale. in one thin translucently white handhe he\n",
            "----------\n",
            "Predicted:      ten  a   en  teen  t n  the t  e  ae te   t  ah  a   nn e  e      te te  t     ta  n  taneaong \n",
            "True:      ld a handkerchief while with the other he stroked the delicatemustache he had grown moving his finge\n",
            "----------\n",
            "Predicted:    h    ee hen hhe  ah    an ahe  n ahe  t          t   n  aen ahn  tn  ae   n  aee t    a  n     ta\n",
            "True:      rs slowly. his eyes gazed at themas they entered.on seeing his face and meeting his eyes princess ma\n",
            "----------\n",
            "Predicted: n   aonn tho    e ee  t    h e to e ae  hh    ao ha  hn  ae  aa t he      tee a            aotn  ean\n",
            "True:      rys pace suddenlyslackened she felt her tears dry up and her sobs ceased. she suddenlyfelt guilty an\n",
            "----------\n",
            "Predicted:   ee   t n   ao th    n  aee a       nn h  ten ton  tnd         tn heen a nanth toe   t e tn    ae  \n",
            "True:      d grew timid on catching the expression of his face andeyes.but in what am i to blame she asked hers\n",
            "----------\n",
            "Predicted:      tn  aen th   hh    ao       n  a       t   an  hn ne tn  ahee  n  ae ahe t n ng hee   tnn nngth\n",
            "True:      elf. and his cold stern lookreplied because you are alive and thinking of the living while i...in th\n",
            "----------\n",
            "Predicted: e te   h n  the  ah     ah he   to  h     e  ao  ang     hhe      an ann    tee  ne t       n  tn he\n",
            "True:      e deep gaze that seemed to look not outwards but inwards therewas an almost hostile expression as he\n",
            "----------\n",
            "Predicted:   e    eth       henhh n    tn      e n  tan    aen aen    te   ng ta  aen  an aonean aan ahe n ta  \n",
            "True:       slowly regarded his sister andnatsha.he kissed his sister holding her hand in his as was their wont\n",
            "----------\n",
            "Predicted: h e  a   ae  han  ae  the h  eten n  th h  hae   tann te tehantoen    he   an  an    tn teneto  t e \n",
            "True:      .how are you mary how did you manage to get here said he in a voiceas calm and aloof as his look.had\n",
            "----------\n",
            "Predicted:  ae a ee     an tn    ahe  ah    eae e  a  hae   te e   ho   te  eeen   a  n     he    he    an hhe \n",
            "True:       he screamed in agony that scream would not have struck such horrorinto princess marys heart as the \n",
            "----------\n",
            "Predicted: ah   au aeneao n      aen  ta  to     haen hn ttnn  tn te tn    hn ahe h n  th  ehahn   n    hnd t n\n",
            "True:      tone of his voice.and have you brought little nicholas he asked in the same slow quietmanner and wit\n",
            "----------\n",
            "Predicted: hetn ae en   a      ah ao          ean  ae  ao eha n t  ng    ta   ae      th e  n   an aaen the t n\n",
            "True:      h an obvious effort to remember.how are you now said princess mary herself surprised at what she was\n",
            "----------\n",
            "Predicted:   een       hoeao   aa  to  han tthe t   h  he ah   n  hn  an  ngtentn  n h  na   a      ah to tn   \n",
            "True:      saying.that my dear you must ask the doctor he replied and again makingan evident effort to be affec\n",
            "----------\n",
            "Predicted: het nt  te t nne an  aen ten  t  e aen      te  ne  aon ao  th         ah hen th  e    e   nte   tn \n",
            "True:      tionate he said with his lips only hiswords clearly did not correspond to his thoughtsmerci chre ami\n",
            "----------\n",
            "Predicted: n t h  te     thaaaaee   ae  to  ao  ng toeah      n     ton  ah      aen ten   the te     e ten  te\n",
            "True:      e dtre venue.      thank you for coming my dear.princess mary pressed his hand. the pressure made hi\n",
            "----------\n",
            "Predicted: n aen       h          e te t n thn    an  h   t n h   hhe  tee  hh hh  nthe to            aee  ae  \n",
            "True:      m wince justperceptibly. he was silent and she did not know what to say. she nowunderstood what had \n",
            "----------\n",
            "Predicted: aen       h henethe thn  to      tntteeeho    aen h   tn  a     nn n anghhe haon ean e   hn hn   nt \n",
            "True:      happened to him two days before. in his words histone and especially in that calm almost antagonisti\n",
            "----------\n",
            "Predicted: nntee tth    ae     an h            to   t   e he g th     ng th th n th    ah  en   ngte  t   t  tn\n",
            "True:      c look could befelt an estrangement from everything belonging to this world terriblein one who is al\n",
            "----------\n",
            "Predicted: l    t e       a    ten etn t      hhn a  t              e   ton ng t n tn te  t   n  tthe  he ta n \n",
            "True:      ive. evidently only with an effort did he understandanything living but it was obvious that he faile\n",
            "----------\n",
            "Predicted:   ah the      d h          te t      the t     ah ao to tht aa      te t          aa                \n",
            "True:      d to understand notbecause he lacked the power to do so but because he understood somethingelsesomet\n",
            "----------\n",
            "Predicted: he g ahe h nen  hhn ho  aa  ao    ao  th      t  n  aeen        hh    n  ten hhng           ao  te  \n",
            "True:      hing the living did not and could not understandand whichwholly occupied his mind.there you see how \n",
            "----------\n",
            "Predicted: aohe      to   te  to    e te hh       ahnnehe     tng hhe t en    t   ao n     ah ao   en the t    \n",
            "True:      strangely fate has brought us together said hebreaking the silence and pointing to natsha. she looks\n",
            "----------\n",
            "Predicted:  aneh  ae annn e thne    ng    tan  ae n  aen tnd aonaaa  ae      n  ae ete t n   a   a    an e    t\n",
            "True:       after me allthe time.princess mary heard him and did not understand how he could say such athing. h\n",
            "----------\n",
            "Predicted: e the a    n    th     a  n   tn     t  et     ae t   ahe        ae  aa   te t     an  aoe a     aon\n",
            "True:      e the sensitive tender prince andrew how could he say thatbefore her whom he loved and who loved him\n",
            "----------\n",
            "Predicted: ehen te ao       ah thn  ae      a   he   t  n the   t     tn the  a      n    a    a     tnnte ten \n",
            "True:       had he expected to live hecould not have said those words in that offensively cold tone. if he hadn\n",
            "----------\n",
            "Predicted:    hoe   a e hae aen t  n  tee ae    ae ae   ta     ah hhne ae ehn     te h  ae a   n a n  th   an h\n",
            "True:      ot known that he was dying how could he have failed to pity her andhow could he speak like that in h\n",
            "----------\n",
            "Predicted: e   o        he ae   a        nn ton  e  he tan an  ntn     ha      t      n  t        eto   t      \n",
            "True:      er presence the only explanation wasthat he was indifferent because something else much more importa\n",
            "----------\n",
            "Predicted: n     a    th   n   ah hen ehe t         n  ten to   an  t nt         an  ae   ng   t h   h t     n \n",
            "True:      nthad been revealed to him.the conversation was cold and disconnected and continually broke off.mary\n",
            "----------\n",
            "Predicted:  hon  teeho  t  the   tenn t n    n  ng  tnd    ton a   th     ahe  aoe aonnn  aan aho    thn  an  a\n",
            "True:       came by way of ryazn said natsha.prince andrew did not notice that she called his sister mary and o\n",
            "----------\n",
            "Predicted: n  h     ae e n  te  ah hn aen ao       a neao   e ao  n  anhaore         n a  hn      e  ah e ae  a\n",
            "True:      nlyafter calling her so in his presence did natsha notice it herself.really he asked.they told her t\n",
            "----------\n",
            "Predicted: henhan  to     ao  ao   te     ao   tnd ahe         e thh      tnhhh  ane    net th hh   ehnhhh  th \n",
            "True:      hat all moscow has been burned down and that...natsha stopped. it was impossible to talk. it was pla\n",
            "----------\n",
            "Predicted: nngaee  ae han antn  tt hoa    ah hen h  te  ao    ao  aa ao     the  aen anh ah     te t nn nhn  an\n",
            "True:      in that he wasmaking an effort to listen but could not do so.yes they say its burned he said. its a \n",
            "----------\n",
            "Predicted: ahe n aone an  ae te           eao     ten tnn      aah   ng hen the hte  ten eten hoe      n  a  ha\n",
            "True:      great pity and he gazedstraight before him absently stroking his mustache with his fingers.and so yo\n",
            "----------\n",
            "Predicted: n t    he  h     aonn     hen  th ng   n     h           n   ne     a n e g thehh    a   n n    aheh\n",
            "True:      u have met count nicholas mary prince andrew suddenly saidevidently wishing to speak pleasantly to t\n",
            "----------\n",
            "Predicted: he  ehe ta    te    he  ae    ahnthee  a nttt thhha  ae ta tha  the  nea   a n n  h  na           th\n",
            "True:      hem. he wrote here that hetook a great liking to you he went on simply and calmly evidentlyunable to\n",
            "----------\n",
            "Predicted: nt          a   the ae      a    n    g  aee th    aen ao   n ng ae   n  t nto  aent  ten th  tnhhh \n",
            "True:       understand all the complex significance his words had forliving people. if you liked him too it wou\n",
            "----------\n",
            "Predicted: t eee tnae   ahen  to  te n  te   en  n  ae tn e  ae  e  aa   aeene   tn atnto      tn ae  n       a\n",
            "True:      ld be a good thing for youto get married he added rather more quickly as if pleased at havingfound w\n",
            "----------\n",
            "Predicted: aae   e aen t    ae   ae  en  e        a    ae    aan aa    a   ahe  ae  ao ae   ng h   ae  a      a\n",
            "True:      ords he had long been seeking.princess mary heard his words but they had no meaning for her except a\n",
            "----------\n",
            "Predicted: n  h ee  te te  hon an   ae to  hen ta   a    ehen  tonen      ahneeae aa aoe t nneaa n    and te   \n",
            "True:      sa proof of how far away he now was from everything living.why talk of me she said quietly and glanc\n",
            "----------\n",
            "Predicted: h  aheton         e te  to   te  a  n e ton h   ta   tn ae   hn  thee  te   t   n  nn          te   \n",
            "True:      ed at natsha.natsha who felt her glance did not look at her. all three were againsilent.andrew would\n",
            "----------\n",
            "Predicted:  ah  ton  nn e en     ten  ao       a  n hn tnth    nn    nn ta    ao  t n  th ah  t n hn hhn      t\n",
            "True:       you like... princess mary suddenly said in a tremblingvoice would you like to see little nicholas h\n",
            "----------\n",
            "Predicted: e tn  n     ah  e       hao e   g   n     a  n   ae   a       n   an  a   ahe aan  hahn  ae    ng   \n",
            "True:      e is always talkingabout youprince andrew smiled just perceptibly and for the first time butprincess\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "# 0) Reverse vocab\n",
        "idx2char = {idx: char for char, idx in vocab.items()}\n",
        "\n",
        "# 1) Evaluation loop\n",
        "tolstoi_char_rnn_custom.eval()\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_dataloader:\n",
        "        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "\n",
        "        # 2) Get the model output and predicted indices\n",
        "        y_pred = tolstoi_char_rnn_custom(X_batch)\n",
        "        print(y_pred.shape)\n",
        "        pred_indices = torch.argmax(y_pred,dim=1)  # (batch_size, seq_length)\n",
        "        print(pred_indices.shape)\n",
        "        # 3) Convert predicted IDs and true IDs to strings\n",
        "        for i in range(len(X_batch)):\n",
        "            pred_string = ''.join(idx2char[idx.item()] for idx in pred_indices[i])\n",
        "            true_string = ''.join(idx2char[idx.item()] for idx in y_batch[i])\n",
        "            \n",
        "            print(\"Predicted:\", pred_string)\n",
        "            print(\"True:     \", true_string)\n",
        "            print(\"----------\")\n",
        "\n",
        "        # Optionally break early to just inspect one batch\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "saved_model = FLNet2()\n",
        "saved_model.load_state_dict(torch.load(load_path, weights_only=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywoEitAZbodq"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(idx_client, _list_trainloader, percentage_to_remove):\n",
        "    list_trainloader = copy.deepcopy(_list_trainloader)\n",
        "    client_dataset = list_trainloader[idx_client].dataset\n",
        "\n",
        "    num_samples_to_remove = int(len(client_dataset) * percentage_to_remove)\n",
        "    num_samples_to_keep = len(client_dataset) - num_samples_to_remove\n",
        "    \n",
        "    if num_samples_to_keep != 0:\n",
        "        new_dataset = TensorDataset(client_dataset[:num_samples_to_keep][0], client_dataset[:num_samples_to_keep][1])\n",
        "        list_trainloader[idx_client] = DataLoader(new_dataset, batch_size=128)\n",
        "\n",
        "    else:\n",
        "        list_trainloader.pop(idx_client)\n",
        "\n",
        "    return list_trainloader, num_samples_to_remove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "def analyze_removal(_model, _trainloader_list):\n",
        "    remove_percentages = np.linspace(0, 1, 10)\n",
        "    trainloader_list = copy.deepcopy(_trainloader_list)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    model = copy.deepcopy(_model).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    # Shuffle training datasets for each client only at the beginning\n",
        "    # (this is done to avoid fictitious determinism in the results)\n",
        "    training_datasets = [loader.dataset for loader in trainloader_list]\n",
        "    shuffled_training_datasets = []\n",
        "    for dataset in training_datasets:\n",
        "        indices = np.random.permutation(len(dataset))\n",
        "        shuffled_training_datasets.append(TensorDataset(dataset[indices][0], dataset[indices][1]))\n",
        "    shuffled_trainloader_list = [DataLoader(dataset, batch_size=128) for dataset in shuffled_training_datasets]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for remove_percentage in remove_percentages:\n",
        "\n",
        "        trainloader_list, _ = create_dataloader(party_to_be_erased, shuffled_trainloader_list, remove_percentage)\n",
        "        \n",
        "        client_dataset = ConcatDataset([loader.dataset for loader in trainloader_list])\n",
        "        loader = DataLoader(client_dataset, batch_size=128)\n",
        "\n",
        "        loss_sum = 0\n",
        "        grad_sum = torch.zeros_like(torch.cat([param.flatten() for param in model.parameters()]))\n",
        "        number_of_batches = 0\n",
        "\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            # Forward pass\n",
        "            model.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Sum quantities\n",
        "            loss_sum += loss.cpu().item()\n",
        "            grad_sum += torch.cat([param.grad.flatten() for param in model.parameters()])\n",
        "            number_of_batches += 1\n",
        "            \n",
        "        grad_norm = torch.norm(grad_sum).cpu().item() / number_of_batches\n",
        "        loss = loss_sum / number_of_batches\n",
        "\n",
        "        \n",
        "        # Store results\n",
        "        results.append({\n",
        "            'removal_percentage': remove_percentage,\n",
        "            'gradient_norm': grad_norm,\n",
        "            'loss': loss\n",
        "        })\n",
        "        \n",
        "        print(f\"Removal percentage: {remove_percentage:.2f}\")\n",
        "        print(f\"  - Number of samples: {sum([len(loader.dataset) for loader in trainloader_list])}\")\n",
        "        print(f\"  - Gradient norm: {grad_norm:.4f}\")\n",
        "        print(f\"  - Loss: {loss:.4f}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Convert results to DataFrame for easier analysis\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Create plots\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Plot gradient norm vs Removal percentage\n",
        "    axs[0].plot(results_df['removal_percentage'], results_df['gradient_norm'], 'o-')\n",
        "    axs[0].set_title('Gradient Norm vs. Removal Percentage')\n",
        "    axs[0].set_xlabel('Removal Percentage')\n",
        "    axs[0].set_ylabel('Gradient Norm')\n",
        "    axs[0].grid(True)\n",
        "\n",
        "    # Plot loss vs Removal percentage\n",
        "    axs[1].plot(results_df['removal_percentage'], results_df['loss'], 'o-')\n",
        "    axs[1].set_title('Loss vs. Removal Percentage')\n",
        "    axs[1].set_xlabel('Removal Percentage')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#analyze_removal(saved_model, trainloader_lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from backpack import backpack, extend\n",
        "from backpack.extensions import DiagHessian\n",
        "\n",
        "trainloader_list = [DataLoader(dloader.dataset, batch_size=128) for dloader in trainloader_lst]\n",
        "#trainloader_list_removed, num_removed = create_dataloader(party_to_be_erased, trainloader_list, 0.8) \n",
        "\n",
        "#Use the model if coming from the training or load if you have it downloaded\n",
        "final_model = copy.deepcopy(saved_model)\n",
        "final_model = extend(final_model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = extend(criterion)\n",
        "\n",
        "delta = 0.0001\n",
        "\n",
        "# Create weights list\n",
        "weights_unlearned = [1] * len(trainloader_list)\n",
        "weights_unlearned[party_to_be_erased] = 1 - delta\n",
        "\n",
        "weights_unlearned=torch.tensor(weights_unlearned, dtype=torch.float32)\n",
        "\n",
        "weights_half = weights_unlearned\n",
        "weights_half[party_to_be_erased] = 1 - delta / 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute Hessians"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_diag_hessian(model, criterion, inputs, targets, device='cpu'):\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    model.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    with backpack(DiagHessian()):\n",
        "        loss.backward()\n",
        "\n",
        "    diag_hessian_params = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        if hasattr(param, 'diag_h') and param.requires_grad:\n",
        "            diag_hessian_params[name] = param.diag_h.clone().detach()\n",
        "            # Cleanup to avoid leftover references\n",
        "            del param.diag_h\n",
        "\n",
        "    return diag_hessian_params\n",
        "\n",
        "\n",
        "class AccumulatedDiagHessian:\n",
        "\n",
        "    def __init__(self, model, criterion, device=None):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Will store final per-client Hessians after summing across that client's batches\n",
        "        self.client_hessians = []\n",
        "        self.total_batches = 0\n",
        "        self._computed = False\n",
        "\n",
        "    def compute_client_hessians(self, dataloader_list):\n",
        "  \n",
        "        self.model = self.model.to(self.device)\n",
        "        self.client_hessians = []\n",
        "        self.total_batches = 0\n",
        "\n",
        "        for loader_idx, loader in enumerate(dataloader_list):\n",
        "            accumulated_diag_h = {}\n",
        "            print(f\"[Hessian] Calculating diagonal Hessian for client {loader_idx}...\")\n",
        "            \n",
        "            for batch_idx, (inputs, targets) in enumerate(loader):\n",
        "                # Compute the diag Hessian for this batch\n",
        "                diag_h = compute_diag_hessian(self.model, self.criterion, inputs, targets, device=self.device)\n",
        "\n",
        "                # Accumulate\n",
        "                for name, value in diag_h.items():\n",
        "                    if name not in accumulated_diag_h:\n",
        "                        accumulated_diag_h[name] = value\n",
        "                    else:\n",
        "                        accumulated_diag_h[name] += value\n",
        "\n",
        "                self.total_batches += 1\n",
        "\n",
        "            # Store the accumulated Hessian for this client\n",
        "            self.client_hessians.append(accumulated_diag_h)\n",
        "\n",
        "        self._computed = True\n",
        "\n",
        "    def weighted_hessian(self, weights=None):\n",
        "\n",
        "        if not self._computed:\n",
        "            raise RuntimeError(\"You must call `compute_client_hessians(...)` before requesting weights.\")\n",
        "\n",
        "        n_clients = len(self.client_hessians)\n",
        "        if weights is None:\n",
        "            weights = [1.0] * n_clients\n",
        "        if len(weights) != n_clients:\n",
        "            raise ValueError(\"Length of weights must match number of clients.\")\n",
        "\n",
        "        weighted_hessian = {}\n",
        "\n",
        "        for name in self.client_hessians[0].keys():\n",
        "            weighted_hessian[name] = sum(\n",
        "                self.client_hessians[i][name] * weights[i] for i in range(len(self.client_hessians))\n",
        "            ) / self.total_batches\n",
        "        \n",
        "        return weighted_hessian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if COMPUTE_HESSIAN:\n",
        "    hessian_calculator = AccumulatedDiagHessian(final_model, criterion)\n",
        "    hessian_calculator.compute_client_hessians(trainloader_list)\n",
        "    accumulated_diag_h = hessian_calculator.weighted_hessian()\n",
        "    accumulated_diag_h_removed = hessian_calculator.weighted_hessian(weights_unlearned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z00eqTLgZUjz"
      },
      "source": [
        "OUR METHOD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC03hVs6ZW7s"
      },
      "outputs": [],
      "source": [
        "def compute_true_info(hessian, hessian_delta, delta):\n",
        "    A_list = [torch.empty_like(p) for p in hessian.values()]\n",
        "    B_list = [torch.empty_like(p) for p in hessian.values()]\n",
        "    for i,k in enumerate(hessian.keys()): \n",
        "        print(k)\n",
        "\n",
        "        degeneracies_indices = torch.logical_or(hessian[k]==0,hessian_delta[k]==0)\n",
        "        print(f\"Number of degenaracies: {torch.sum(degeneracies_indices)}\")\n",
        "\n",
        "        ratio = hessian_delta[k]/hessian[k]\n",
        "        A=1/2 * torch.log(ratio)/delta\n",
        "        B=1/2 * (1-ratio)/delta\n",
        "\n",
        "        A[degeneracies_indices]=0\n",
        "        B[degeneracies_indices]=0\n",
        "\n",
        "        assert torch.sum(torch.isnan(A))==0, f\"NaN values in A: {torch.sum(torch.isnan(A))}\"\n",
        "        assert torch.sum(torch.isnan(B))==0, f\"NaN values in B: {torch.sum(torch.isnan(B))}\"\n",
        "        assert torch.sum(torch.isinf(A))==0, f\"Inf values in A: {torch.sum(torch.isinf(A))}\"\n",
        "        assert torch.sum(torch.isinf(B))==0, f\"Inf values in B: {torch.sum(torch.isinf(B))}\"\n",
        "\n",
        "        A_list[i]=A\n",
        "        B_list[i]=B\n",
        "\n",
        "    C = sum([torch.sum(A_list[i]+B_list[i]).item() for i in range(len(A_list))])\n",
        "\n",
        "    information_true=[(A_list[i]+B_list[i])*C + 2*torch.pow(B_list[i],2) for i in range(len(hessian.keys()))]\n",
        "\n",
        "    return information_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_second_deriv_info(hessian, hessian_half_delta, hessian_delta, delta):\n",
        "    information = []\n",
        "    for k in hessian.keys(): \n",
        "        print(k)\n",
        "        degeneracies_indices = torch.logical_or(hessian[k]==0,hessian_delta[k]==0)\n",
        "        degeneracies_indices = torch.logical_or(degeneracies_indices, torch.pow(hessian_half_delta[k],2)==0)\n",
        "        print(f\"Number of degenaracies: {torch.sum(degeneracies_indices)}\")\n",
        "\n",
        "        A=1/2 * torch.log(hessian_delta[k]*hessian[k]/torch.pow(hessian_half_delta[k],2))\n",
        "        #plt.plot(A.cpu().flatten(), label=k+'A')\n",
        "        B=1/2 * (2*hessian_half_delta[k]/hessian[k]-hessian_delta[k]/hessian[k] -1)\n",
        "        print(np.var(B.cpu().flatten().numpy()))\n",
        "        print(np.mean(B.cpu().flatten().numpy()))\n",
        "        #plt.plot(B.cpu().flatten(), label=k+'B')\n",
        "\n",
        "        #plt.legend()\n",
        "        #plt.show()\n",
        "\n",
        "        I=-(A+B)/((delta/2)**2)\n",
        "\n",
        "        I[degeneracies_indices]=0\n",
        "\n",
        "        assert torch.sum(torch.isnan(I))==0, f\"NaN values in {k}: {torch.sum(torch.isnan(I))}\"\n",
        "        assert torch.sum(torch.isinf(I))==0, f\"Inf values in {k}: {torch.sum(torch.isinf(I))}\"\n",
        "\n",
        "        information.append(I)\n",
        "\n",
        "    return information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def delta_hessian(hessian, hessian_delta, delta):\n",
        "    delta_hessian = []\n",
        "    for k in hessian.keys():\n",
        "        delta_hessian.append(torch.abs(hessian_delta[k] - hessian[k]) / delta)\n",
        "    return delta_hessian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mgeUVILJZbnd",
        "outputId": "29f4735c-7cd4-4709-8179-b44b3728eef7"
      },
      "outputs": [],
      "source": [
        "if COMPUTE_HESSIAN:  \n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    info_true = compute_true_info(accumulated_diag_h, accumulated_diag_h_removed, torch.as_tensor([delta]).to(device))\n",
        "    #info_true = compute_second_deriv_info(accumulated_diag_h, accumulated_diag_h_half, accumulated_diag_h_removed, torch.as_tensor([delta]).to(device))\n",
        "    #info_true = delta_hessian(accumulated_diag_h, accumulated_diag_h_removed, torch.as_tensor([delta]).to(device))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if COMPUTE_HESSIAN:\n",
        "    import pickle\n",
        "\n",
        "    with open('cached/info_true.pkl', 'wb') as f:\n",
        "        pickle.dump(info_true, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load hessians"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('cached/info_true.pkl', 'rb') as f:\n",
        "    info_true = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ModxW028mf5Y"
      },
      "outputs": [],
      "source": [
        "def sum_information_values(information):\n",
        "    # Initialize an empty list to store all flattened tensors\n",
        "    all_values = []\n",
        "\n",
        "    # Iterate through each parameter's information tensor\n",
        "    for tensor in information:\n",
        "        # Flatten the tensor and add to our list\n",
        "        temp=torch.nan_to_num(tensor.flatten())\n",
        "        temp = temp[torch.isinf(temp)==0]\n",
        "        temp = temp[temp!=0]\n",
        "        all_values.append(temp)\n",
        "\n",
        "    # Concatenate all tensors into one large tensor\n",
        "    combined_tensor = torch.cat(all_values)\n",
        "    print(len(combined_tensor))\n",
        "    return combined_tensor\n",
        "\n",
        "def plot_information(information):\n",
        "    combined_tensor = sum_information_values(information)\n",
        "\n",
        "    # Convert to numpy for plotting\n",
        "    values = combined_tensor.detach().cpu().numpy()\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    #Histogram\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(values, alpha=0.7, bins=50)\n",
        "    plt.title('Distribution of Information Values')\n",
        "    plt.xlabel('Value')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    # Exclude outliers for the second plot (optional)\n",
        "    q1, q3 = np.percentile(values, [0, 98])\n",
        "    iqr = q3 - q1\n",
        "    # lower_bound = q1 - 1.5 * iqr\n",
        "    # upper_bound = q3 + 1.5 * iqr\n",
        "    lower_bound = q1\n",
        "    upper_bound = q3\n",
        "    filtered_values = values[(values >= lower_bound) & (values <= upper_bound)]\n",
        "    print(f\"Number of outliers: {len(values) - len(filtered_values)}\")\n",
        "\n",
        "    # Filtered histogram (without outliers)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(filtered_values, alpha=0.7, bins=30, color='green')\n",
        "    plt.title('Distribution (Outliers Removed)')\n",
        "    plt.xlabel('Value')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print some statistics\n",
        "    print(f\"Sum of all information values: {combined_tensor.sum().item():.6f}\")\n",
        "    print(f\"Mean: {combined_tensor.mean().item()}\")\n",
        "    print(f\"Min: {combined_tensor.min().item()}\")\n",
        "    print(f\"Max: {combined_tensor.max().item()}\")\n",
        "    print(f\"Total number of values: {len(values)}\")\n",
        "\n",
        "    print(f\"Sum of all information values: {filtered_values.sum().item():.6f}\")\n",
        "    print(f\"Mean: {filtered_values.mean().item()}\")\n",
        "    print(f\"Min: {filtered_values.min().item()}\")\n",
        "    print(f\"Max: {filtered_values.max().item()}\")\n",
        "    print(f\"Total number of values: {len(filtered_values)}\")\n",
        "\n",
        "    return combined_tensor.sum().item(), filtered_values.sum().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "F4R6HHhMmlt5",
        "outputId": "2c8b6c11-1b48-4702-86be-64b04f0bb913"
      },
      "outputs": [],
      "source": [
        "plot_information(info_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "import seaborn as sns\n",
        "\n",
        "# create validation routine\n",
        "def validate(net, dl, n_classes, device):\n",
        "    # create metric objects\n",
        "    tm_acc = torchmetrics.Accuracy(task='multiclass', num_classes=n_classes, average= 'macro', top_k=1)\n",
        "    tm_con = torchmetrics.ConfusionMatrix(task=\"multiclass\", num_classes=n_classes)\n",
        "    # move metric to device\n",
        "    net.to(device)\n",
        "    tm_acc.to(device)\n",
        "    tm_con.to(device)\n",
        "    # set network in eval mode\n",
        "    net.eval()\n",
        "    # at the end of epoch, validate model\n",
        "    for loader_idx, loader in enumerate(dl):\n",
        "        for inputs, targets in loader:\n",
        "        \n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            # remove singleton dimension\n",
        "            targets = targets.squeeze()\n",
        "            # get output\n",
        "            with torch.no_grad():\n",
        "                # perform prediction\n",
        "                logits = net(inputs)\n",
        "            # update metrics\n",
        "            _, predicted = torch.max(logits.data, 1)\n",
        "            tm_acc.update(predicted, targets)\n",
        "            tm_con.update(predicted, targets)\n",
        "\n",
        "    # at the end, compute metric\n",
        "    acc = tm_acc.compute()\n",
        "    con = tm_con.compute()\n",
        "    # set network in training mode\n",
        "    \n",
        "    return acc, con\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(conf_mat):\n",
        "    cm = sns.light_palette(\"blue\", as_cmap=True)\n",
        "    x=pd.DataFrame(conf_mat.cpu())\n",
        "    x=x.style.background_gradient(cmap=cm)\n",
        "    display(x)\n",
        "\n",
        "\n",
        "# Compute accuracy for each client\n",
        "def compute_accuracy(model, testloader):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = model.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def compute_accuracy_per_client(model, testloader_list):\n",
        "    for i, testloader in enumerate(testloader_list):\n",
        "        acc = compute_accuracy(model, testloader)\n",
        "        print(f\"Accuracy for class {i}: {acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "info_true=[p.cpu() for p in info_true]\n",
        "infocat = torch.cat([p.flatten() for p in info_true])\n",
        "s_infocat = np.sort(torch.nan_to_num(infocat))[::-1]\n",
        "\n",
        "s_infolayers = [np.sort(p.cpu().flatten())[::-1] for p in info_true]\n",
        "\n",
        "plt.plot(s_infocat)\n",
        "plt.title('Sorted Information Values for All Parameters')\n",
        "plt.show()\n",
        "\n",
        "# from kneed import KneeLocator\n",
        "# start = [0, 0, 0, 0, 0, 0, 0, 0]\n",
        "# end = [-1, -1, -10000, -30, -1, -1, -1, -1]\n",
        "# step = [1, 1, 1, 1, 100, 1, 1, 1]\n",
        "# auto_thresholds = []\n",
        "# for i in range(len(s_infolayers)):\n",
        "#     kneedle = KneeLocator(range(len(s_infolayers[i][start[i]:end[i]:step[i]])), s_infolayers[i][start[i]:end[i]:step[i]], curve='convex', direction='decreasing', online=True)\n",
        "#     auto_thresholds.append(start[i]+kneedle.knee*step[i])\n",
        "#     print(f'Layer {i} knee:', auto_thresholds[i])\n",
        "\n",
        "auto_percentage = 12\n",
        "custom_percentages = [None, None, None, None, None, None, None, None]\n",
        "\n",
        "percentages = [auto_percentage if custom_percentages[i] is None else custom_percentages[i] for i in range(len(custom_percentages))]\n",
        "\n",
        "auto_thresholds = [len(s_infolayers[i]) // 100 * percentages[i] for i in range(len(s_infolayers))]\n",
        "\n",
        "custom_thresholds = [None, None, None, None, None, None, None, None]\n",
        "\n",
        "thresholds = [auto_thresholds[i] if custom_thresholds[i] is None else custom_thresholds[i] for i in range(len(custom_thresholds))]\n",
        "\n",
        "for i in range(len(s_infolayers)):\n",
        "    fig, axs = plt.subplots(1, 1, figsize=(14, 5))\n",
        "    axs.plot(s_infolayers[i])\n",
        "    axs.axvline(auto_thresholds[i], color='red', linestyle='--', label='Auto threshold')\n",
        "    axs.axvline(thresholds[i], color='green', linestyle='--', label='Custom threshold')\n",
        "    axs.legend()\n",
        "    axs.set_title(f'Thresholds for layer {i}')\n",
        "    fig.show()\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parameters reset unlearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "thresholds_values = [p[thresholds[i]] for i,p in enumerate(s_infolayers)]\n",
        "retrain_param_indices = []\n",
        "for i, p in enumerate(info_true):\n",
        "    if custom_thresholds[i] != 0:\n",
        "        indices = torch.argwhere(p > thresholds_values[i])\n",
        "        retrain_param_indices.append(indices)\n",
        "    else:\n",
        "        retrain_param_indices.append(torch.empty(size=[0 for _ in range(p.dim())]))\n",
        "\n",
        "model_reset = copy.deepcopy(saved_model).cpu()\n",
        "model_null = FLNet2()\n",
        "\n",
        "original_parameters = model_reset.state_dict()\n",
        "null_parameters = model_null.state_dict()\n",
        "\n",
        "reset_parameters = {}\n",
        "for i, (name, p) in enumerate(model_reset.named_parameters()):\n",
        "    new_p = original_parameters[name].clone()\n",
        "    null_p = null_parameters[name].clone()\n",
        "    #new_p[tuple(retrain_param_indices[i].t())] = null_p[tuple(retrain_param_indices[i].t())]\n",
        "    new_p[tuple(retrain_param_indices[i].t())] = 0\n",
        "    reset_parameters[name] = new_p\n",
        "\n",
        "model_reset.load_state_dict(reset_parameters)\n",
        "\n",
        "print(\"Accuracy before resetting:\")\n",
        "compute_accuracy_per_client(saved_model, trainloader_class_list)\n",
        "print(\"Accuracy after resetting:\")\n",
        "compute_accuracy_per_client(model_reset, trainloader_class_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Confusion matrix before resetting:\")\n",
        "acc, conf_mat = validate(saved_model, trainloader_class_list, 10, device)\n",
        "plot_confusion_matrix(conf_mat)\n",
        "print(\"Confusion matrix after resetting:\")\n",
        "acc, conf_mat = validate(model_reset, trainloader_class_list, 10, device)\n",
        "plot_confusion_matrix(conf_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_info_stats(information):\n",
        "    \"\"\"\n",
        "    Compute statistics for the information values.\n",
        "    \"\"\"\n",
        "    # Flatten the information values and convert to numpy\n",
        "    layers_total_information = [torch.sum(p) for p in information]\n",
        "\n",
        "    for i, info in enumerate(layers_total_information):\n",
        "        print(f\"Layer {i} - Total Information: {info:.4f}\")\n",
        "    \n",
        "    print(f\"Total Information: {sum(layers_total_information):.4f}\")\n",
        "\n",
        "print(\"Information before resetting:\")\n",
        "print_info_stats(info_true)\n",
        "\n",
        "reset_info_true = copy.deepcopy(info_true)\n",
        "for i in range(len(reset_info_true)):\n",
        "    reset_info_true[i][tuple(retrain_param_indices[i].t())] = 0\n",
        "print(\"Information after resetting:\")\n",
        "print_info_stats(reset_info_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UnlearnNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A module that wraps an existing model and selectively retrains individual \n",
        "    scalar elements (indices) of its parameters while keeping the rest fixed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_model, indices_to_retrain):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            base_model (nn.Module): The original model whose parameters \n",
        "                                    we want to partially retrain.\n",
        "            indices_to_retrain (List[torch.Tensor]): For each parameter of \n",
        "                                    'base_model', a tensor of indices indicating \n",
        "                                    which scalar values should be retrained.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # We store the base model inside a dictionary to allow\n",
        "        # functional calls later without overshadowing state_dict keys.\n",
        "        self.inner_model = {\"model\": base_model}\n",
        "\n",
        "        # Move any index tensors to CPU and store them.\n",
        "        self.indices_to_retrain = [idx.cpu() for idx in indices_to_retrain]\n",
        "\n",
        "        # Create a copy of the base model's parameters as buffers, where\n",
        "        # we zero out the positions that will be retrained.\n",
        "        base_params = {}\n",
        "        for i, (param_name, param) in enumerate(base_model.named_parameters()):\n",
        "            # Detach a clone of the original parameter\n",
        "            cloned_param = param.clone().detach()\n",
        "            # Zero-out the entries we plan to retrain\n",
        "            if len(self.indices_to_retrain[i]) > 0:\n",
        "                cloned_param[tuple(self.indices_to_retrain[i].t())] = 0\n",
        "            base_params[param_name] = cloned_param\n",
        "\n",
        "        # Register these base parameters as buffers so they are not optimized\n",
        "        for param_name, buf in base_params.items():\n",
        "            buf_name = param_name.replace(\".\", \"_\")\n",
        "            self.register_buffer(f\"base_{buf_name}\", buf)\n",
        "\n",
        "        # Create the new learnable parameters for only the chosen indices\n",
        "        retrain_params_dict = {}\n",
        "        for i, (param_name, param) in enumerate(base_model.named_parameters()):\n",
        "            if len(self.indices_to_retrain[i]) == 0:\n",
        "                continue\n",
        "            # We create a 1D tensor (one entry per retrained element)\n",
        "            key = param_name.replace(\".\", \"_\")\n",
        "            retrain_params_dict[key] = nn.Parameter(\n",
        "                torch.zeros(len(self.indices_to_retrain[i]))\n",
        "            )\n",
        "        self.retrain_params = nn.ParameterDict(retrain_params_dict)\n",
        "\n",
        "        # Build sparse masks to apply the learnable values at the correct indices\n",
        "        sparse_masks = {}\n",
        "        for i, (param_name, param) in enumerate(base_model.named_parameters()):\n",
        "            if len(self.indices_to_retrain[i]) == 0:\n",
        "                continue\n",
        "            # 'retrain_indices' has shape (k, n_dims). Add a final dim to index positions in the retrain-param vector.\n",
        "            retrain_indices = indices_to_retrain[i]\n",
        "            k = retrain_indices.size(0)\n",
        "\n",
        "            # Create an index column [0..k-1], then concatenate it with 'retrain_indices'.\n",
        "            row_idx = torch.arange(k).unsqueeze(1)\n",
        "            final_idx_matrix = torch.cat([retrain_indices, row_idx], dim=1)\n",
        "\n",
        "            # A sparse_coo_tensor expects indices with shape (ndim, nnz). Transpose to (n_dims+1, k).\n",
        "            indices_for_sparse = final_idx_matrix.t().contiguous()\n",
        "\n",
        "            # Append k as the final dimension so each retrained element indexes differently.\n",
        "            mask_shape = tuple(param.size()) + (k,)\n",
        "\n",
        "            # Build the sparse mask with 1.0 at the retrained indices.\n",
        "            key = f\"mask_{param_name.replace('.', '_')}\"\n",
        "            sparse_masks[key] = torch.sparse_coo_tensor(\n",
        "                indices_for_sparse,\n",
        "                torch.ones(k, dtype=torch.float32),\n",
        "                size=mask_shape\n",
        "            )\n",
        "        \n",
        "        # Register these sparse masks as buffers\n",
        "        for mask_name, mask in sparse_masks.items():\n",
        "            self.register_buffer(mask_name, mask.coalesce())\n",
        "\n",
        "    def contract_last_dim_with_vector(self, sp_tensor: torch.Tensor, vec: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Contract the last dimension of a sparse tensor (shape [..., N]) with\n",
        "        a dense vector of shape (N,), returning a sparse tensor of shape [...].\n",
        "\n",
        "        This effectively applies elementwise multiplication with 'vec'\n",
        "        across the last dimension of 'sp_tensor'.\n",
        "        \"\"\"\n",
        "\n",
        "        # Extract indices (shape [ndim, nnz]) and values (shape [nnz])\n",
        "        indices = sp_tensor.indices()\n",
        "        values = sp_tensor.values()\n",
        "\n",
        "        # Multiply each sparse value by the corresponding element in 'vec'\n",
        "        # indices[-1] indicates which element in 'vec' to use per sparse entry\n",
        "        new_values = values * vec[indices[-1]]\n",
        "\n",
        "        # Create a new sparse_coo_tensor with one fewer dimension\n",
        "        new_shape = sp_tensor.shape[:-1]\n",
        "        new_indices = indices[:-1, :]  # drop the last dimension index row\n",
        "\n",
        "        result_tensor = torch.sparse_coo_tensor(\n",
        "            new_indices,\n",
        "            new_values,\n",
        "            size=new_shape,\n",
        "            dtype=sp_tensor.dtype,\n",
        "            device=sp_tensor.device\n",
        "        )\n",
        "\n",
        "        return result_tensor\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass using a functional call to the base model. We reconstruct \n",
        "        final parameters by adding the base buffers and the contracted retrain \n",
        "        parameters at the relevant indices.\n",
        "        \"\"\"\n",
        "        model = self.inner_model[\"model\"]\n",
        "        current_state = self.state_dict()\n",
        "\n",
        "        # Rebuild parameter dict from buffers (base params)\n",
        "        final_params = {}\n",
        "        for param_name in model.state_dict().keys():\n",
        "            buf_name = param_name.replace(\".\", \"_\")\n",
        "            final_params[param_name] = current_state[f\"base_{buf_name}\"]\n",
        "\n",
        "        # Add in the learnable values at specified indices\n",
        "        for key, param_vector in self.retrain_params.items():\n",
        "            mask_key = f\"mask_{key}\"\n",
        "            base_key = f\"base_{key}\"\n",
        "            original_param_name = key.replace(\"_\", \".\")\n",
        "\n",
        "            # Convert sparse mask to shape that can be added to base param\n",
        "            sparse_update = self.contract_last_dim_with_vector(\n",
        "                current_state[mask_key], param_vector\n",
        "            )\n",
        "\n",
        "            # Add the sparse update to the base buffer\n",
        "            final_params[original_param_name] = (\n",
        "                current_state[base_key] + sparse_update\n",
        "            )\n",
        "\n",
        "        # Perform a functional forward pass with the reconstructed parameters\n",
        "        return torch.func.functional_call(model, final_params, x)\n",
        "    \n",
        "    def get_retrained_params(self):\n",
        "        \"\"\"\n",
        "        Returns the retrained parameters of the model.\n",
        "        \"\"\"\n",
        "        model = self.inner_model[\"model\"]\n",
        "        current_state = self.state_dict()\n",
        "\n",
        "        # Rebuild parameter dict from buffers (base params)\n",
        "        final_params = {}\n",
        "        for param_name in model.state_dict().keys():\n",
        "            buf_name = param_name.replace(\".\", \"_\")\n",
        "            final_params[param_name] = current_state[f\"base_{buf_name}\"]\n",
        "\n",
        "        # Add in the learnable values at specified indices\n",
        "        for key, param_vector in self.retrain_params.items():\n",
        "            mask_key = f\"mask_{key}\"\n",
        "            base_key = f\"base_{key}\"\n",
        "            original_param_name = key.replace(\"_\", \".\")\n",
        "\n",
        "            # Convert sparse mask to shape that can be added to base param\n",
        "            sparse_update = self.contract_last_dim_with_vector(\n",
        "                current_state[mask_key], param_vector\n",
        "            )\n",
        "\n",
        "            # Add the sparse update to the base buffer\n",
        "            final_params[original_param_name] = (\n",
        "                current_state[base_key] + sparse_update\n",
        "            )\n",
        "        \n",
        "        detached_params = {}\n",
        "        for key, value in final_params.items():\n",
        "            detached_params[key] = value.cpu().detach()\n",
        "        return detached_params\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert RETRAIN\n",
        "#Test\n",
        "unlearn_model = UnlearnNet(model_reset, retrain_param_indices)\n",
        "\n",
        "print(\"Unlearn model parameters:\")\n",
        "for name , param in unlearn_model.named_parameters():\n",
        "    print(name, param.shape)\n",
        "\n",
        "# print(\"Accuracy (reset before retrain)\")\n",
        "# compute_accuracy_per_client(unlearn_model, trainloader_class_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_of_repeats = 1\n",
        "num_fl_rounds = 1\n",
        "\n",
        "#fusion_types = ['FedAvg','Retrain']\n",
        "fusion_types = ['FedAvg']\n",
        "fusion_types_unlearn = ['Retrain', 'Unlearn']\n",
        "\n",
        "num_updates_in_epoch = None\n",
        "num_local_epochs = 2\n",
        "\n",
        "dist_Retrain = {}\n",
        "loss_fed = {}\n",
        "grad_fed = {}\n",
        "clean_accuracy = {}\n",
        "pois_accuracy = {}\n",
        "for fusion_key in fusion_types:\n",
        "    loss_fed[fusion_key] = np.zeros(num_fl_rounds)\n",
        "    grad_fed[fusion_key] = np.zeros(num_fl_rounds)\n",
        "    clean_accuracy[fusion_key] = np.zeros(num_fl_rounds)\n",
        "    pois_accuracy[fusion_key] = np.zeros(num_fl_rounds)\n",
        "    if fusion_key != 'Retrain':\n",
        "        dist_Retrain[fusion_key] = np.zeros(num_fl_rounds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if device == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "party_models_dict = {}\n",
        "initial_model = unlearn_model\n",
        "model_dict = {}\n",
        "parties = list(range(5))\n",
        "del parties[party_to_be_erased]\n",
        "\n",
        "for fusion_key in fusion_types:\n",
        "    model_dict[fusion_key] = copy.deepcopy(initial_model.state_dict())\n",
        "\n",
        "for round_num in range(num_fl_rounds):\n",
        "    local_training = LocalTraining(num_updates_in_epoch=num_updates_in_epoch, num_local_epochs=num_local_epochs)\n",
        "\n",
        "    for fusion_key in fusion_types:\n",
        "        fusion = FL_round_fusion_selection(num_parties=len(parties), fusion_key=fusion_key)\n",
        "\n",
        "        current_model_state_dict = copy.deepcopy(model_dict[fusion_key])\n",
        "        model = copy.deepcopy(initial_model)\n",
        "        model.load_state_dict(current_model_state_dict)\n",
        "\n",
        "        ##################### Local Training Round #############################\n",
        "        party_models = []\n",
        "        party_losses = []\n",
        "        party_grad   = []        \n",
        "        for party_id in parties:\n",
        "\n",
        "            if fusion_key == 'Retrain':\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Training party {party_id}\")\n",
        "                model = UnlearnNet(model_reset, retrain_param_indices)\n",
        "                model.load_state_dict(current_model_state_dict)\n",
        "                model.to(device)\n",
        "                model_update, party_loss = local_training.train(model=model,\n",
        "                                            trainloader=trainloader_lst[party_id],\n",
        "                                            device = device,\n",
        "                                            criterion=None, opt=None)\n",
        "                model_update_copy = UnlearnNet(model_reset, retrain_param_indices)\n",
        "                model_update_copy.load_state_dict(model_update.state_dict())\n",
        "                model_update_copy.to(\"cpu\")\n",
        "                party_models.append(model_update_copy)\n",
        "                party_losses.append(party_loss)\n",
        "\n",
        "            grad_norm = 0.0\n",
        "            for name,param in model_update.named_parameters():\n",
        "                if param.grad is not None:\n",
        "                    grad_norm += torch.norm(param.grad).cpu().item()\n",
        "                    #print(f\"Gradient norm for {name}: {grad_norm}\")\n",
        "                else:\n",
        "                    #print(f\"Gradient is None for {name}\")\n",
        "                    pass\n",
        "        \n",
        "            party_grad.append(grad_norm)   \n",
        "        \n",
        "        grad_fed[fusion_key][round_num] += (np.mean(party_grad)/num_of_repeats) \n",
        "\n",
        "        loss_fed[fusion_key][round_num] += (np.mean(party_losses)/num_of_repeats)\n",
        "        ######################################################################\n",
        "\n",
        "        current_model_state_dict = fusion.fusion_algo(party_models=party_models, current_model=model)\n",
        "\n",
        "        eval_model = copy.deepcopy(initial_model).to(\"cpu\")\n",
        "        eval_model.load_state_dict(current_model_state_dict)\n",
        "        clean_acc = Utils.evaluate(testloader, eval_model)\n",
        "        clean_accuracy[fusion_key][round_num] = clean_acc\n",
        "        print(f'Global Clean Accuracy {fusion_key}, round {round_num} = {clean_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Accuracy after retrain\")\n",
        "compute_accuracy_per_client(eval_model, trainloader_class_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Confusion matrix after retrain:\")\n",
        "acc, conf_mat = validate(eval_model, trainloader_class_list, 10, \"cuda\")\n",
        "plot_confusion_matrix(conf_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainloader_list = [DataLoader(dloader.dataset, batch_size=128) for dloader in trainloader_lst]\n",
        "#trainloader_list_removed, num_removed = create_dataloader(party_to_be_erased, trainloader_list, 0.8) \n",
        "\n",
        "\n",
        "final_model = FLNet2()\n",
        "unlearned_params = eval_model.get_retrained_params()\n",
        "\n",
        "\n",
        "final_model.load_state_dict(unlearned_params)\n",
        "final_model = extend(final_model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = extend(criterion)\n",
        "\n",
        "delta = 0.01\n",
        "\n",
        "# Create weights list\n",
        "weights_unlearned = [1] * len(trainloader_list)\n",
        "weights_unlearned[party_to_be_erased] = 0\n",
        "weights_delta = weights_unlearned.copy()\n",
        "weights_delta[party_to_be_erased] = delta\n",
        "\n",
        "weights_unlearned=torch.tensor(weights_unlearned, dtype=torch.float32)\n",
        "weights_delta=torch.tensor(weights_delta, dtype=torch.float32)\n",
        "\n",
        "accumulated_diag_h_retrain = calculate_accumulated_diag_hessian(trainloader_list, final_model, criterion, weights_unlearned)\n",
        "accumulated_diag_h_removed_retrain = calculate_accumulated_diag_hessian(trainloader_list, final_model, criterion, weights_delta)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "info_true = compute_true_info(accumulated_diag_h_retrain, accumulated_diag_h_removed_retrain, torch.as_tensor([delta]).to(device))\n",
        "\n",
        "plot_information(info_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client0 = torch.load(\"client0\")\n",
        "client1 = torch.load(\"client1\")\n",
        "client2 = torch.load(\"client2\")\n",
        "client3 = torch.load(\"client3\")\n",
        "client4 = torch.load(\"client4\")\n",
        "\n",
        "# For NumPy arrays\n",
        "import numpy as np\n",
        "\n",
        "def count_equal_arrays(list1, list2):\n",
        "    equal_count = 0\n",
        "    min_length = min(len(list1), len(list2))\n",
        "    \n",
        "    for i in range(min_length):\n",
        "        if np.array_equal(list1[i], list2[i]):\n",
        "            print(list1[i])\n",
        "            equal_count += 1\n",
        "    \n",
        "    return equal_count\n",
        "\n",
        "count_equal_arrays(client4[6], client4[6])\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def tensor_list_distance(list1, list2, metric='euclidean'):\n",
        "    \"\"\"\n",
        "    Compute various distance metrics between two lists of tensors.\n",
        "    \n",
        "    Args:\n",
        "        list1: First list of tensors\n",
        "        list2: Second list of tensors\n",
        "        metric: Distance metric to use ('euclidean', 'manhattan', 'cosine', 'mean_squared', 'element_diff')\n",
        "        \n",
        "    Returns:\n",
        "        float: The computed distance between the two tensor lists\n",
        "    \"\"\"\n",
        "    # Make sure we only compare up to the length of the shorter list\n",
        "    min_length = min(len(list1), len(list2))\n",
        "    \n",
        "    if min_length == 0:\n",
        "        raise ValueError(\"At least one of the lists is empty\")\n",
        "        \n",
        "    # Initialize distance\n",
        "    distances = []\n",
        "    \n",
        "    # Compute distance for each pair of tensors\n",
        "    for i in range(min_length):\n",
        "        tensor1 = list1[i].float()  # Ensure tensors are float type\n",
        "        tensor2 = list2[i].float()\n",
        "        \n",
        "        if metric == 'euclidean':\n",
        "            # Euclidean distance\n",
        "            dist = torch.sqrt(torch.sum((tensor1 - tensor2) ** 2)).item()\n",
        "            distances.append(dist)\n",
        "            \n",
        "        elif metric == 'manhattan':\n",
        "            # Manhattan (L1) distance\n",
        "            dist = torch.sum(torch.abs(tensor1 - tensor2)).item()\n",
        "            distances.append(dist)\n",
        "            \n",
        "        elif metric == 'cosine':\n",
        "            # Cosine distance (1 - cosine similarity)\n",
        "            dot_product = torch.sum(tensor1 * tensor2)\n",
        "            norm1 = torch.sqrt(torch.sum(tensor1 ** 2))\n",
        "            norm2 = torch.sqrt(torch.sum(tensor2 ** 2))\n",
        "            \n",
        "            # Avoid division by zero\n",
        "            if norm1 == 0 or norm2 == 0:\n",
        "                dist = 1.0  # Maximum distance\n",
        "            else:\n",
        "                cosine_sim = dot_product / (norm1 * norm2)\n",
        "                # Clip to handle floating point errors\n",
        "                cosine_sim = torch.clamp(cosine_sim, -1.0, 1.0)\n",
        "                dist = 1.0 - cosine_sim.item()\n",
        "                \n",
        "            distances.append(dist)\n",
        "            \n",
        "        elif metric == 'mean_squared':\n",
        "            # Mean squared error\n",
        "            dist = torch.mean((tensor1 - tensor2) ** 2).item()\n",
        "            distances.append(dist)\n",
        "            \n",
        "        elif metric == 'element_diff':\n",
        "            # Count of different elements\n",
        "            diff_count = torch.sum(tensor1 != tensor2).item()\n",
        "            distances.append(diff_count)\n",
        "            \n",
        "        else:\n",
        "            raise ValueError(f\"Unknown distance metric: {metric}\")\n",
        "    \n",
        "    # Compute average distance across all tensor pairs\n",
        "    avg_distance = np.mean(distances)\n",
        "    \n",
        "    # Calculate additional statistics\n",
        "    stats = {\n",
        "        \"mean\": avg_distance,\n",
        "        \"median\": np.median(distances),\n",
        "        \"min\": np.min(distances),\n",
        "        \"max\": np.max(distances),\n",
        "        \"std\": np.std(distances),\n",
        "        \"distances\": distances  # List of all pairwise distances\n",
        "    }\n",
        "    \n",
        "    return stats\n",
        "\n",
        "    \n",
        "# Compute distances using different metrics\n",
        "for metric in ['euclidean', 'element_diff']:\n",
        "    result = tensor_list_distance(client0[4], client1[4], metric=metric)\n",
        "    print(f\"\\n{metric.upper()} Distance:\")\n",
        "    print(f\"Mean: {result['mean']:.6f}\")\n",
        "    print(f\"Median: {result['median']:.6f}\")\n",
        "    print(f\"Min: {result['min']:.6f}\")\n",
        "    print(f\"Max: {result['max']:.6f}\")\n",
        "    print(f\"Std: {result['std']:.6f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9GexoT5UbGQl"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
